{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Conv1D, Input,MaxPooling1D,Flatten,LeakyReLU,AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "import random\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from keras import regularizers\n",
    "from keras.metrics import binary_accuracy\n",
    "from sklearn.metrics import confusion_matrix,recall_score,matthews_corrcoef,roc_curve,roc_auc_score,auc,precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau,LearningRateScheduler\n",
    "import os, sys, copy, getopt, re, argparse\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import tensorflow as tf\n",
    "from keras import losses\n",
    "import pickle\n",
    "\n",
    "from scipy import interp\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "import keras\n",
    "from keras.initializers import RandomUniform\n",
    "import keras.backend as K\n",
    "from random import shuffle\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(temp, OutputDir):\n",
    "\n",
    "    trainning_result, validation_result, testing_result = temp\n",
    "\n",
    "    file = open(OutputDir + '/performance.txt', 'w')\n",
    "\n",
    "    index = 0\n",
    "    for x in [trainning_result, validation_result, testing_result]:\n",
    "\n",
    "\n",
    "        title = ''\n",
    "\n",
    "        if index == 0:\n",
    "            title = 'training_'\n",
    "        if index == 1:\n",
    "            title = 'validation_'\n",
    "        if index == 2:\n",
    "            title = 'testing_'\n",
    "\n",
    "        index += 1;\n",
    "\n",
    "        file.write(title +  'results\\n')\n",
    "\n",
    "\n",
    "        for j in ['sn', 'sp', 'acc', 'MCC','AUC', 'precision', 'F1', 'lossValue']: \n",
    "\n",
    "            total = []\n",
    "\n",
    "            for val in x:\n",
    "                total.append(val[j])\n",
    "            file.write(j + ' : mean : ' + str(np.mean(total)) + ' std : ' + str(np.std(total))  + '\\n')\n",
    "\n",
    "        file.write('\\n\\n______________________________\\n')\n",
    "    file.close()\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for x in [trainning_result, validation_result, testing_result]:\n",
    "\n",
    "        tprs = []\n",
    "        aucs = []\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        \n",
    "        i = 0\n",
    "\n",
    "        for val in x:\n",
    "            tpr = val['tpr']\n",
    "            fpr = val['fpr']\n",
    "            tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "            tprs[-1][0] = 0.0\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            plt.plot(fpr, tpr, lw=1, alpha=0.3,label='ROC fold %d (AUC = %0.2f)' % (i+1, roc_auc))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Random', alpha=.8)\n",
    "\n",
    "        mean_tpr = np.mean(tprs, axis=0)\n",
    "        mean_tpr[-1] = 1.0\n",
    "        mean_auc = auc(mean_fpr, mean_tpr)\n",
    "        std_auc = np.std(aucs)\n",
    "        plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "                 label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "                 lw=2, alpha=.8)\n",
    "\n",
    "        std_tpr = np.std(tprs, axis=0)\n",
    "        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "        plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                         label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        title = ''\n",
    "\n",
    "        if index == 0:\n",
    "            title = 'training_'\n",
    "        if index == 1:\n",
    "            title = 'validation_'\n",
    "        if index == 2:\n",
    "            title = 'testing_'\n",
    "\n",
    "        plt.savefig( OutputDir + '/' + title +'ROC.png')\n",
    "        plt.close('all');\n",
    "        \n",
    "       #************************** Precision Recall Curve*********************************\n",
    "        i = 0\n",
    "        prs = []\n",
    "        pre_aucs = []\n",
    "        mean_recal= np.linspace(0, 1, 100)\n",
    "        for val in x:\n",
    "            pre = val['prec']\n",
    "            rec = val['reca']\n",
    "            prs.append(interp(mean_recal, rec, pre))\n",
    "            prs[-1][0] = 0.0\n",
    "            p_r_auc = auc(rec, pre)\n",
    "            pre_aucs.append(p_r_auc)\n",
    "            plt.plot(rec, pre, lw=1, alpha=0.3,label='PRC fold %d (AUC = %0.2f)' % (i+1, p_r_auc))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        mean_pre = np.mean(prs, axis=0)\n",
    "        mean_auc = auc(mean_recal, mean_pre)\n",
    "        std_auc = np.std(pre_aucs)\n",
    "        plt.plot(mean_recal, mean_pre, color='b',\n",
    "                 label=r'Mean PRC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "                 lw=2, alpha=.8)\n",
    "\n",
    "        std_pre = np.std(prs, axis=0)\n",
    "        pre_upper = np.minimum(mean_pre + std_pre, 1)\n",
    "        pre_lower = np.maximum(mean_pre - std_pre, 0)\n",
    "        plt.fill_between(mean_recal, pre_lower, pre_upper, color='grey', alpha=.2,\n",
    "                         label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision Recall curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        title = ''\n",
    "\n",
    "        if index == 0:\n",
    "            title = 'training_'\n",
    "        if index == 1:\n",
    "            title = 'validation_'\n",
    "        if index == 2:\n",
    "            title = 'testing_'\n",
    "\n",
    "        plt.savefig( OutputDir + '/' + title +'Pre_R_C.png')\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "        index += 1\n",
    "\n",
    "def chunkIt(seq, num):\n",
    "    avg = len(seq) / float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "#nucleotide chemical properties and frequency encoding\n",
    "def cal(c,cb,i):\n",
    "    bases ={'A':[1,1,1], 'C':[0,1,0], 'G':[1,0,0,], 'T':[0,0,1]}\n",
    "    p=[]\n",
    "    p=bases[c]\n",
    "    p.append(np.round(cb/float(i+1),2))\n",
    "    return(p)\n",
    "def calculate(s):\n",
    "    p=f=list()\n",
    "    cba=cbc=cbt=cbg=0\n",
    "    for i,c in enumerate(s):\n",
    "        if c=='A':\n",
    "            cba+=1\n",
    "            p=cal(c,cba,i)\n",
    "        elif c=='T':\n",
    "            cbt+=1\n",
    "            p=cal(c,cbt,i)\n",
    "        elif c=='C':\n",
    "            cbc+=1\n",
    "            p=cal(c,cbc,i)\n",
    "        elif c=='G':\n",
    "            cbg+=1\n",
    "            p=cal(c,cbg,i)\n",
    "        else:\n",
    "            p=[0,0,0,0]\n",
    "        f.append(p)\n",
    "    return(f)\n",
    "\n",
    "def dataProcessing(seq,key):\n",
    "    X_chem = []\n",
    "    for a in range(len(seq)):\n",
    "        X_chem.append(calculate(seq[a]))\n",
    "    X_chem = np.array(X_chem)\n",
    "    #print(X_chem.shape)\n",
    "\n",
    "                \n",
    "\n",
    "    if key == 1:\n",
    "        lbs = list(np.ones(len(X_chem)))\n",
    "    if key == 2:\n",
    "        lbs=list(np.zeros(len(X_chem)))\n",
    "    y = np.array(lbs, dtype = np.int32)\n",
    " \n",
    "    return X_chem, y\n",
    "\n",
    "def prepareData(path):\n",
    "    all_seq = []\n",
    "    for seq_record in SeqIO.parse(path, \"fasta\"):\n",
    "        all_seq.append(str(seq_record.seq))\n",
    "    pos_seq = []\n",
    "    neg_seq = []\n",
    "    for i in range(len(all_seq)):\n",
    "        if(i < (len(all_seq)/2)):\n",
    "            pos_seq.append(all_seq[i])\n",
    "        else:\n",
    "            neg_seq.append(all_seq[i])\n",
    "    a=1\n",
    "    b=2\n",
    "    \n",
    "    Positive_X, Positive_y = dataProcessing(pos_seq,a)\n",
    "    Negitive_X, Negitive_y = dataProcessing(neg_seq,b)\n",
    "\n",
    "    return Positive_X, Positive_y, Negitive_X, Negitive_y\n",
    "\n",
    "def shuffleData(X, y):\n",
    "    index = [i for i in range(len(X))]\n",
    "    random.shuffle(index)\n",
    "    X = X[index]\n",
    "    y = y[index]\n",
    "    return X, y\n",
    "\n",
    "def getMode():\n",
    "\n",
    "    input_shape = (41,4)\n",
    "\n",
    "    inputs = Input(shape = input_shape)\n",
    "\n",
    "    convLayer = Conv1D(filters = 8, kernel_size = 3,padding='same',kernel_regularizer = regularizers.l2(1e-3),bias_regularizer = regularizers.l2(1e-4),activation = 'relu',input_shape = input_shape)(inputs);\n",
    "    normalizationLayer=BatchNormalization()(convLayer)\n",
    "    convLayer2 = Conv1D(filters = 16, kernel_size = 3,padding='same',kernel_regularizer = regularizers.l2(1e-3),bias_regularizer = regularizers.l2(1e-4),activation = 'relu',input_shape = input_shape)(normalizationLayer)\n",
    "    flattenLayer = Flatten()(convLayer2)\n",
    "    denseLayer = Dense(16, activation = 'relu',kernel_regularizer = regularizers.l2(1e-3),bias_regularizer = regularizers.l2(1e-4))(flattenLayer)\n",
    "    dropoutLayer = Dropout(0.3)(denseLayer)\n",
    "    outLayer = Dense(1, activation='sigmoid')(dropoutLayer)\n",
    "    model = Model(inputs = inputs, outputs = outLayer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer= Adam(lr=0.001), metrics=[binary_accuracy]); #SGD(lr = 0.005, momentum=0.95)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def calculateScore(X, y, model):\n",
    "    \n",
    "    score = model.evaluate(X,y)\n",
    "    pred_y = model.predict(X)\n",
    "\n",
    "    accuracy = score[1];\n",
    "\n",
    "    tempLabel = np.zeros(shape = y.shape, dtype=np.int32)\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if pred_y[i] < 0.5:\n",
    "            tempLabel[i] = 0\n",
    "        else:\n",
    "            tempLabel[i] = 1\n",
    "    confusion = confusion_matrix(y, tempLabel)\n",
    "    TN, FP, FN, TP = confusion.ravel()\n",
    "\n",
    "    sensitivity = recall_score(y, tempLabel)\n",
    "    specificity = TN / float(TN+FP)\n",
    "    MCC = matthews_corrcoef(y, tempLabel)\n",
    "\n",
    "    F1Score = (2 * TP) / float(2 * TP + FP + FN)\n",
    "    precision = TP / float(TP + FP)\n",
    "    recall = TP/float (TP+FN)\n",
    "    pred_y = pred_y.reshape((-1, ))\n",
    "\n",
    "    ROCArea = roc_auc_score(y, pred_y)\n",
    "    fpr, tpr, thresholds = roc_curve(y, pred_y)\n",
    "    lossValue = None\n",
    "    \n",
    "    pre, rec, threshlds = precision_recall_curve(y, pred_y)\n",
    "    pre = np.fliplr([pre])[0] \n",
    "    rec = np.fliplr([rec])[0]  \n",
    "    AUC_prec_rec = np.trapz(rec,pre)\n",
    "    AUC_prec_rec = abs(AUC_prec_rec)\n",
    "    \n",
    "    print(y.shape)\n",
    "    print(pred_y.shape)\n",
    "\n",
    "    y_true = tf.convert_to_tensor(y, np.float32)\n",
    "    y_pred = tf.convert_to_tensor(pred_y, np.float32)\n",
    "\n",
    "    with tf.Session():\n",
    "        lossValue = losses.binary_crossentropy(y_true, y_pred).eval()  \n",
    "    plt.show()\n",
    "    return {'sn' : sensitivity, 'sp' : specificity, 'acc' : accuracy, 'MCC' : MCC, 'AUC' : ROCArea,'precision' : precision, 'F1' : F1Score, 'fpr' : fpr, 'tpr' : tpr, 'thresholds' : thresholds, 'lossValue' : lossValue,'pre_recall_curve':AUC_prec_rec,'prec':pre,'reca':rec}\n",
    "def test_data_prepro(ind_test):\n",
    "    Positive_X, Positive_y, Negitive_X, Negitive_y = prepareData(ind_test)\n",
    "    test_X = np.concatenate((Positive_X,Negitive_X))\n",
    "    test_y = np.concatenate((Positive_y,Negitive_y))\n",
    "    return test_X, test_y\n",
    "def funciton(All_data, OutputDir, folds):\n",
    "\n",
    "    Positive_X, Positive_y, Negitive_X, Negitive_y = prepareData(All_data)\n",
    "    random.shuffle(Positive_X);\n",
    "    random.shuffle(Negitive_X);\n",
    "    Positive_X_Slices = chunkIt(Positive_X, folds)\n",
    "    Positive_y_Slices = chunkIt(Positive_y, folds)\n",
    "    Negative_X_Slices = chunkIt(Negitive_X, folds)\n",
    "    Negative_y_Slices = chunkIt(Negitive_y, folds)\n",
    "\n",
    "    trainning_result = []\n",
    "    validation_result = []\n",
    "    testing_result = []\n",
    "    for test_index in range(folds):\n",
    "\n",
    "        test_X = np.concatenate((Positive_X_Slices[test_index],Negative_X_Slices[test_index]))\n",
    "        test_y = np.concatenate((Positive_y_Slices[test_index],Negative_y_Slices[test_index]))\n",
    "        \n",
    "        validation_index = (test_index+1) % folds;\n",
    "\n",
    "        valid_X = np.concatenate((Positive_X_Slices[validation_index],Negative_X_Slices[validation_index]))\n",
    "        valid_y = np.concatenate((Positive_y_Slices[validation_index],Negative_y_Slices[validation_index]))\n",
    "\n",
    "        start = 0\n",
    "\n",
    "        for val in range(0, folds):\n",
    "            if val != test_index and val != validation_index:\n",
    "                start = val\n",
    "                break\n",
    "\n",
    "        train_X = np.concatenate((Positive_X_Slices[start],Negative_X_Slices[start]))\n",
    "        train_y = np.concatenate((Positive_y_Slices[start],Negative_y_Slices[start]))\n",
    "\n",
    "        for i in range(0, folds):\n",
    "            if i != test_index and i != validation_index and i != start:\n",
    "                tempX = np.concatenate((Positive_X_Slices[i],Negative_X_Slices[i]))\n",
    "                tempy = np.concatenate((Positive_y_Slices[i],Negative_y_Slices[i]))\n",
    "\n",
    "                \n",
    "                train_X = np.concatenate((train_X, tempX))\n",
    "                train_y = np.concatenate((train_y, tempy))\n",
    "\n",
    "    \n",
    "        test_X, test_y = shuffleData(test_X,test_y)\n",
    "        valid_X,valid_y = shuffleData(valid_X,valid_y)\n",
    "        train_X,train_y = shuffleData(train_X,train_y)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'x_test',test_X)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'y_test',test_y)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'valid_X',valid_X)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'valid_y',valid_y)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'x_train',train_X)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'y_train',train_y)\n",
    "        \n",
    "        \n",
    "        model = getMode()\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience= 10, restore_best_weights=True)\n",
    "        model_check = ModelCheckpoint(filepath = OutputDir + \"/model\" + str(test_index+1) +\".h5\",mode='min', monitor = 'val_loss', save_best_only=True)#, save_weights_only=True\n",
    "        reduct_L_rate = ReduceLROnPlateau(monitor='val_loss',factor=0.01, patience=10)\n",
    "        cbacks = [model_check,reduct_L_rate,early_stopping]\n",
    "        \n",
    "        history = model.fit(train_X, train_y, batch_size = 16, epochs =200,validation_data = (valid_X, valid_y),callbacks = cbacks);\n",
    "        model=load_model('C:/Users/NSCL/N4-methylctosine//outputs//model'+str(test_index+1)+'.h5')\n",
    "        \n",
    "        trainning_result.append(calculateScore(train_X, train_y, model))\n",
    "        validation_result.append(calculateScore(valid_X, valid_y, model))\n",
    "        testing_result.append(calculateScore(test_X, test_y, model))\n",
    "\n",
    "    temp_dict = (trainning_result, validation_result, testing_result)\n",
    "    analyze(temp_dict, OutputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 41, 16)            400       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 656)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                10512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 11,065\n",
      "Trainable params: 11,049\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1448 samples, validate on 182 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1448/1448 [==============================] - 1s 981us/step - loss: 0.6894 - binary_accuracy: 0.6257 - val_loss: 0.5304 - val_binary_accuracy: 0.7857\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 201us/step - loss: 0.4779 - binary_accuracy: 0.8122 - val_loss: 0.3588 - val_binary_accuracy: 0.8791\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 197us/step - loss: 0.3677 - binary_accuracy: 0.8791 - val_loss: 0.2336 - val_binary_accuracy: 0.9505\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 178us/step - loss: 0.3070 - binary_accuracy: 0.8888 - val_loss: 0.2178 - val_binary_accuracy: 0.9560\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 200us/step - loss: 0.2683 - binary_accuracy: 0.9227 - val_loss: 0.1797 - val_binary_accuracy: 0.9725\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 193us/step - loss: 0.2203 - binary_accuracy: 0.9427 - val_loss: 0.1700 - val_binary_accuracy: 0.9835\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 184us/step - loss: 0.2039 - binary_accuracy: 0.9496 - val_loss: 0.1514 - val_binary_accuracy: 0.9835\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 220us/step - loss: 0.1721 - binary_accuracy: 0.9655 - val_loss: 0.1347 - val_binary_accuracy: 0.9835\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 323us/step - loss: 0.1699 - binary_accuracy: 0.9606 - val_loss: 0.1279 - val_binary_accuracy: 0.9835\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 288us/step - loss: 0.1603 - binary_accuracy: 0.9689 - val_loss: 0.1259 - val_binary_accuracy: 0.9890\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 205us/step - loss: 0.1339 - binary_accuracy: 0.9814 - val_loss: 0.1138 - val_binary_accuracy: 0.9890\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 196us/step - loss: 0.1393 - binary_accuracy: 0.9724 - val_loss: 0.1125 - val_binary_accuracy: 0.9945\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 209us/step - loss: 0.1254 - binary_accuracy: 0.9800 - val_loss: 0.1018 - val_binary_accuracy: 0.9780\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 200us/step - loss: 0.1090 - binary_accuracy: 0.9869 - val_loss: 0.1336 - val_binary_accuracy: 0.9780\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 205us/step - loss: 0.1185 - binary_accuracy: 0.9820 - val_loss: 0.1214 - val_binary_accuracy: 0.9890\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 191us/step - loss: 0.1069 - binary_accuracy: 0.9841 - val_loss: 0.0944 - val_binary_accuracy: 0.9890\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 0s 206us/step - loss: 0.0960 - binary_accuracy: 0.9924 - val_loss: 0.1226 - val_binary_accuracy: 0.9835\n",
      "Epoch 18/200\n",
      "1448/1448 [==============================] - 0s 199us/step - loss: 0.1009 - binary_accuracy: 0.9862 - val_loss: 0.1012 - val_binary_accuracy: 0.9780\n",
      "Epoch 19/200\n",
      "1448/1448 [==============================] - 0s 202us/step - loss: 0.1016 - binary_accuracy: 0.9862 - val_loss: 0.0985 - val_binary_accuracy: 0.9835\n",
      "Epoch 20/200\n",
      "1448/1448 [==============================] - 0s 209us/step - loss: 0.0903 - binary_accuracy: 0.9903 - val_loss: 0.1262 - val_binary_accuracy: 0.9670\n",
      "Epoch 21/200\n",
      "1448/1448 [==============================] - 0s 186us/step - loss: 0.0954 - binary_accuracy: 0.9883 - val_loss: 0.0892 - val_binary_accuracy: 0.9890\n",
      "Epoch 22/200\n",
      "1448/1448 [==============================] - 0s 211us/step - loss: 0.0914 - binary_accuracy: 0.9924 - val_loss: 0.0959 - val_binary_accuracy: 0.9890\n",
      "Epoch 23/200\n",
      "1448/1448 [==============================] - 0s 205us/step - loss: 0.0908 - binary_accuracy: 0.9924 - val_loss: 0.0999 - val_binary_accuracy: 0.9835\n",
      "Epoch 24/200\n",
      "1448/1448 [==============================] - 0s 180us/step - loss: 0.0763 - binary_accuracy: 0.9959 - val_loss: 0.1119 - val_binary_accuracy: 0.9835\n",
      "Epoch 25/200\n",
      "1448/1448 [==============================] - 0s 191us/step - loss: 0.0832 - binary_accuracy: 0.9896 - val_loss: 0.1016 - val_binary_accuracy: 0.9890\n",
      "Epoch 26/200\n",
      "1448/1448 [==============================] - 0s 187us/step - loss: 0.0883 - binary_accuracy: 0.9903 - val_loss: 0.1197 - val_binary_accuracy: 0.9835\n",
      "Epoch 27/200\n",
      "1448/1448 [==============================] - 0s 202us/step - loss: 0.0831 - binary_accuracy: 0.9910 - val_loss: 0.0943 - val_binary_accuracy: 0.9835\n",
      "Epoch 28/200\n",
      "1448/1448 [==============================] - 0s 209us/step - loss: 0.0884 - binary_accuracy: 0.9876 - val_loss: 0.1631 - val_binary_accuracy: 0.9780\n",
      "Epoch 29/200\n",
      "1448/1448 [==============================] - 0s 198us/step - loss: 0.0758 - binary_accuracy: 0.9959 - val_loss: 0.0930 - val_binary_accuracy: 0.9890\n",
      "Epoch 30/200\n",
      "1448/1448 [==============================] - 0s 199us/step - loss: 0.0870 - binary_accuracy: 0.9876 - val_loss: 0.1156 - val_binary_accuracy: 0.9890\n",
      "Epoch 31/200\n",
      "1448/1448 [==============================] - 0s 236us/step - loss: 0.0756 - binary_accuracy: 0.9924 - val_loss: 0.0948 - val_binary_accuracy: 0.9835\n",
      "1448/1448 [==============================] - 0s 161us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "182/182 [==============================] - 0s 77us/step\n",
      "(182,)\n",
      "(182,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 0s 83us/step\n",
      "(180,)\n",
      "(180,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 41, 16)            400       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 656)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                10512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 11,065\n",
      "Trainable params: 11,049\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1448 samples, validate on 180 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 2s 1ms/step - loss: 0.6634 - binary_accuracy: 0.6402 - val_loss: 0.4723 - val_binary_accuracy: 0.8611\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 183us/step - loss: 0.4679 - binary_accuracy: 0.8191 - val_loss: 0.3515 - val_binary_accuracy: 0.8778\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 185us/step - loss: 0.3611 - binary_accuracy: 0.8715 - val_loss: 0.3023 - val_binary_accuracy: 0.9000\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 198us/step - loss: 0.3041 - binary_accuracy: 0.9061 - val_loss: 0.2391 - val_binary_accuracy: 0.9167\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 185us/step - loss: 0.2645 - binary_accuracy: 0.9206 - val_loss: 0.2018 - val_binary_accuracy: 0.9444\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 178us/step - loss: 0.2223 - binary_accuracy: 0.9378 - val_loss: 0.1931 - val_binary_accuracy: 0.9500\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 201us/step - loss: 0.1962 - binary_accuracy: 0.9551 - val_loss: 0.2342 - val_binary_accuracy: 0.9444\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 186us/step - loss: 0.1830 - binary_accuracy: 0.9579 - val_loss: 0.1669 - val_binary_accuracy: 0.9500\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 206us/step - loss: 0.1703 - binary_accuracy: 0.9627 - val_loss: 0.1731 - val_binary_accuracy: 0.9556\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 193us/step - loss: 0.1503 - binary_accuracy: 0.9751 - val_loss: 0.1526 - val_binary_accuracy: 0.9556\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 201us/step - loss: 0.1572 - binary_accuracy: 0.9682 - val_loss: 0.1678 - val_binary_accuracy: 0.9500\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 209us/step - loss: 0.1390 - binary_accuracy: 0.9751 - val_loss: 0.1725 - val_binary_accuracy: 0.9611\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 267us/step - loss: 0.1271 - binary_accuracy: 0.9751 - val_loss: 0.1630 - val_binary_accuracy: 0.9722\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 317us/step - loss: 0.1238 - binary_accuracy: 0.9786 - val_loss: 0.1667 - val_binary_accuracy: 0.9556\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 222us/step - loss: 0.1173 - binary_accuracy: 0.9834 - val_loss: 0.1681 - val_binary_accuracy: 0.9500\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 198us/step - loss: 0.1066 - binary_accuracy: 0.9848 - val_loss: 0.1652 - val_binary_accuracy: 0.9611\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 0s 185us/step - loss: 0.1077 - binary_accuracy: 0.9834 - val_loss: 0.1640 - val_binary_accuracy: 0.9722\n",
      "Epoch 18/200\n",
      "1448/1448 [==============================] - 0s 187us/step - loss: 0.1087 - binary_accuracy: 0.9862 - val_loss: 0.1549 - val_binary_accuracy: 0.9722\n",
      "Epoch 19/200\n",
      "1448/1448 [==============================] - 0s 185us/step - loss: 0.1009 - binary_accuracy: 0.9896 - val_loss: 0.1600 - val_binary_accuracy: 0.9611\n",
      "Epoch 20/200\n",
      "1448/1448 [==============================] - 0s 199us/step - loss: 0.0941 - binary_accuracy: 0.9910 - val_loss: 0.1503 - val_binary_accuracy: 0.9722\n",
      "Epoch 21/200\n",
      "1448/1448 [==============================] - 0s 179us/step - loss: 0.0982 - binary_accuracy: 0.9869 - val_loss: 0.1532 - val_binary_accuracy: 0.9722\n",
      "Epoch 22/200\n",
      "1448/1448 [==============================] - 0s 211us/step - loss: 0.0944 - binary_accuracy: 0.9890 - val_loss: 0.1458 - val_binary_accuracy: 0.9611\n",
      "Epoch 23/200\n",
      "1448/1448 [==============================] - 0s 209us/step - loss: 0.0988 - binary_accuracy: 0.9848 - val_loss: 0.1598 - val_binary_accuracy: 0.9722\n",
      "Epoch 24/200\n",
      "1448/1448 [==============================] - 0s 214us/step - loss: 0.0845 - binary_accuracy: 0.9917 - val_loss: 0.1585 - val_binary_accuracy: 0.9778\n",
      "Epoch 25/200\n",
      "1448/1448 [==============================] - 0s 222us/step - loss: 0.0911 - binary_accuracy: 0.9896 - val_loss: 0.1804 - val_binary_accuracy: 0.9611\n",
      "Epoch 26/200\n",
      "1448/1448 [==============================] - 0s 231us/step - loss: 0.0960 - binary_accuracy: 0.9890 - val_loss: 0.1857 - val_binary_accuracy: 0.9611\n",
      "Epoch 27/200\n",
      "1448/1448 [==============================] - 0s 199us/step - loss: 0.0881 - binary_accuracy: 0.9924 - val_loss: 0.1467 - val_binary_accuracy: 0.9556\n",
      "Epoch 28/200\n",
      "1448/1448 [==============================] - 0s 211us/step - loss: 0.0864 - binary_accuracy: 0.9917 - val_loss: 0.1638 - val_binary_accuracy: 0.9611\n",
      "Epoch 29/200\n",
      "1448/1448 [==============================] - 0s 222us/step - loss: 0.0869 - binary_accuracy: 0.9917 - val_loss: 0.1767 - val_binary_accuracy: 0.9611\n",
      "Epoch 30/200\n",
      "1448/1448 [==============================] - 0s 240us/step - loss: 0.0740 - binary_accuracy: 0.9965 - val_loss: 0.1877 - val_binary_accuracy: 0.9556\n",
      "Epoch 31/200\n",
      "1448/1448 [==============================] - 0s 196us/step - loss: 0.0782 - binary_accuracy: 0.9924 - val_loss: 0.1478 - val_binary_accuracy: 0.9667\n",
      "Epoch 32/200\n",
      "1448/1448 [==============================] - 0s 211us/step - loss: 0.0817 - binary_accuracy: 0.9903 - val_loss: 0.1468 - val_binary_accuracy: 0.9611\n",
      "1448/1448 [==============================] - 0s 210us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "180/180 [==============================] - 0s 66us/step\n",
      "(180,)\n",
      "(180,)\n",
      "182/182 [==============================] - 0s 66us/step\n",
      "(182,)\n",
      "(182,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 41, 16)            400       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 656)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                10512     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 11,065\n",
      "Trainable params: 11,049\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1448 samples, validate on 182 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 2s 1ms/step - loss: 0.6546 - binary_accuracy: 0.6692 - val_loss: 0.5124 - val_binary_accuracy: 0.7967\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 209us/step - loss: 0.4751 - binary_accuracy: 0.8080 - val_loss: 0.3733 - val_binary_accuracy: 0.8516\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 200us/step - loss: 0.3797 - binary_accuracy: 0.8715 - val_loss: 0.4227 - val_binary_accuracy: 0.8516\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 208us/step - loss: 0.3361 - binary_accuracy: 0.8950 - val_loss: 0.2751 - val_binary_accuracy: 0.9066\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 222us/step - loss: 0.2841 - binary_accuracy: 0.9192 - val_loss: 0.2530 - val_binary_accuracy: 0.9341\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 195us/step - loss: 0.2451 - binary_accuracy: 0.9309 - val_loss: 0.2463 - val_binary_accuracy: 0.9341\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 215us/step - loss: 0.2224 - binary_accuracy: 0.9420 - val_loss: 0.2357 - val_binary_accuracy: 0.9451\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 183us/step - loss: 0.2040 - binary_accuracy: 0.9482 - val_loss: 0.2165 - val_binary_accuracy: 0.9505\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 191us/step - loss: 0.1919 - binary_accuracy: 0.9503 - val_loss: 0.2131 - val_binary_accuracy: 0.9396\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 205us/step - loss: 0.1824 - binary_accuracy: 0.9579 - val_loss: 0.2081 - val_binary_accuracy: 0.9560\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 264us/step - loss: 0.1698 - binary_accuracy: 0.9662 - val_loss: 0.1969 - val_binary_accuracy: 0.9505\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 288us/step - loss: 0.1504 - binary_accuracy: 0.9682 - val_loss: 0.2039 - val_binary_accuracy: 0.9505\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 209us/step - loss: 0.1367 - binary_accuracy: 0.9786 - val_loss: 0.1986 - val_binary_accuracy: 0.9615\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 181us/step - loss: 0.1255 - binary_accuracy: 0.9779 - val_loss: 0.1916 - val_binary_accuracy: 0.9780\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 189us/step - loss: 0.1310 - binary_accuracy: 0.9793 - val_loss: 0.1938 - val_binary_accuracy: 0.9615\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 178us/step - loss: 0.1150 - binary_accuracy: 0.9841 - val_loss: 0.1974 - val_binary_accuracy: 0.9615\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 0s 207us/step - loss: 0.1119 - binary_accuracy: 0.9834 - val_loss: 0.1962 - val_binary_accuracy: 0.9725\n",
      "Epoch 18/200\n",
      "1448/1448 [==============================] - 0s 187us/step - loss: 0.1096 - binary_accuracy: 0.9820 - val_loss: 0.1885 - val_binary_accuracy: 0.9670\n",
      "Epoch 19/200\n",
      "1448/1448 [==============================] - 0s 188us/step - loss: 0.1037 - binary_accuracy: 0.9883 - val_loss: 0.1883 - val_binary_accuracy: 0.9615\n",
      "Epoch 20/200\n",
      "1448/1448 [==============================] - 0s 208us/step - loss: 0.0978 - binary_accuracy: 0.9896 - val_loss: 0.1923 - val_binary_accuracy: 0.9670\n",
      "Epoch 21/200\n",
      "1448/1448 [==============================] - 0s 236us/step - loss: 0.0987 - binary_accuracy: 0.9876 - val_loss: 0.2012 - val_binary_accuracy: 0.9670\n",
      "Epoch 22/200\n",
      "1448/1448 [==============================] - 0s 187us/step - loss: 0.0985 - binary_accuracy: 0.9834 - val_loss: 0.1900 - val_binary_accuracy: 0.9725\n",
      "Epoch 23/200\n",
      "1448/1448 [==============================] - 0s 180us/step - loss: 0.0976 - binary_accuracy: 0.9876 - val_loss: 0.1889 - val_binary_accuracy: 0.9780\n",
      "Epoch 24/200\n",
      "1448/1448 [==============================] - 0s 200us/step - loss: 0.0938 - binary_accuracy: 0.9883 - val_loss: 0.2052 - val_binary_accuracy: 0.9780\n",
      "Epoch 25/200\n",
      "1448/1448 [==============================] - 0s 210us/step - loss: 0.0840 - binary_accuracy: 0.9924 - val_loss: 0.2088 - val_binary_accuracy: 0.9670\n",
      "Epoch 26/200\n",
      "1448/1448 [==============================] - 0s 184us/step - loss: 0.0976 - binary_accuracy: 0.9848 - val_loss: 0.2126 - val_binary_accuracy: 0.9670\n",
      "Epoch 27/200\n",
      "1448/1448 [==============================] - 0s 196us/step - loss: 0.0969 - binary_accuracy: 0.9834 - val_loss: 0.1777 - val_binary_accuracy: 0.9780\n",
      "Epoch 28/200\n",
      "1448/1448 [==============================] - 0s 191us/step - loss: 0.0885 - binary_accuracy: 0.9890 - val_loss: 0.1859 - val_binary_accuracy: 0.9780\n",
      "Epoch 29/200\n",
      "1448/1448 [==============================] - 0s 202us/step - loss: 0.0881 - binary_accuracy: 0.9910 - val_loss: 0.2292 - val_binary_accuracy: 0.9615\n",
      "Epoch 30/200\n",
      "1448/1448 [==============================] - 0s 183us/step - loss: 0.0986 - binary_accuracy: 0.9841 - val_loss: 0.1945 - val_binary_accuracy: 0.9725\n",
      "Epoch 31/200\n",
      "1448/1448 [==============================] - 0s 185us/step - loss: 0.0837 - binary_accuracy: 0.9896 - val_loss: 0.2052 - val_binary_accuracy: 0.9670\n",
      "Epoch 32/200\n",
      "1448/1448 [==============================] - 0s 209us/step - loss: 0.0848 - binary_accuracy: 0.9903 - val_loss: 0.2200 - val_binary_accuracy: 0.9670\n",
      "Epoch 33/200\n",
      "1448/1448 [==============================] - 0s 192us/step - loss: 0.0831 - binary_accuracy: 0.9910 - val_loss: 0.2344 - val_binary_accuracy: 0.9560\n",
      "Epoch 34/200\n",
      "1448/1448 [==============================] - 0s 185us/step - loss: 0.0759 - binary_accuracy: 0.9945 - val_loss: 0.2484 - val_binary_accuracy: 0.9670\n",
      "Epoch 35/200\n",
      "1448/1448 [==============================] - 0s 185us/step - loss: 0.0774 - binary_accuracy: 0.9924 - val_loss: 0.2225 - val_binary_accuracy: 0.9670\n",
      "Epoch 36/200\n",
      "1448/1448 [==============================] - 0s 196us/step - loss: 0.0762 - binary_accuracy: 0.9917 - val_loss: 0.2301 - val_binary_accuracy: 0.9670\n",
      "Epoch 37/200\n",
      "1448/1448 [==============================] - 0s 185us/step - loss: 0.0856 - binary_accuracy: 0.9890 - val_loss: 0.2201 - val_binary_accuracy: 0.9560\n",
      "1448/1448 [==============================] - 0s 272us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "182/182 [==============================] - 0s 66us/step\n",
      "(182,)\n",
      "(182,)\n",
      "180/180 [==============================] - 0s 55us/step\n",
      "(180,)\n",
      "(180,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 41, 16)            400       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 656)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                10512     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 11,065\n",
      "Trainable params: 11,049\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1448 samples, validate on 180 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 2s 1ms/step - loss: 0.6321 - binary_accuracy: 0.6706 - val_loss: 0.4789 - val_binary_accuracy: 0.8278\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 1s 389us/step - loss: 0.4393 - binary_accuracy: 0.8267 - val_loss: 0.3788 - val_binary_accuracy: 0.8167\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 258us/step - loss: 0.3536 - binary_accuracy: 0.8757 - val_loss: 0.3124 - val_binary_accuracy: 0.8778\n",
      "Epoch 4/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1448/1448 [==============================] - 0s 192us/step - loss: 0.3031 - binary_accuracy: 0.8999 - val_loss: 0.2473 - val_binary_accuracy: 0.9444\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 179us/step - loss: 0.2634 - binary_accuracy: 0.9247 - val_loss: 0.2238 - val_binary_accuracy: 0.9444\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 186us/step - loss: 0.2271 - binary_accuracy: 0.9316 - val_loss: 0.1858 - val_binary_accuracy: 0.9500\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 185us/step - loss: 0.1968 - binary_accuracy: 0.9448 - val_loss: 0.1790 - val_binary_accuracy: 0.9611\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 197us/step - loss: 0.1803 - binary_accuracy: 0.9634 - val_loss: 0.1628 - val_binary_accuracy: 0.9556\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 197us/step - loss: 0.1669 - binary_accuracy: 0.9620 - val_loss: 0.1756 - val_binary_accuracy: 0.9611\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 218us/step - loss: 0.1616 - binary_accuracy: 0.9641 - val_loss: 0.1883 - val_binary_accuracy: 0.9389\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 192us/step - loss: 0.1431 - binary_accuracy: 0.9751 - val_loss: 0.1421 - val_binary_accuracy: 0.9556\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 191us/step - loss: 0.1328 - binary_accuracy: 0.9765 - val_loss: 0.1286 - val_binary_accuracy: 0.9611\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 192us/step - loss: 0.1274 - binary_accuracy: 0.9779 - val_loss: 0.1384 - val_binary_accuracy: 0.9667\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 185us/step - loss: 0.1264 - binary_accuracy: 0.9786 - val_loss: 0.1373 - val_binary_accuracy: 0.9778\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 194us/step - loss: 0.1130 - binary_accuracy: 0.9814 - val_loss: 0.1242 - val_binary_accuracy: 0.9667\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 192us/step - loss: 0.1082 - binary_accuracy: 0.9876 - val_loss: 0.1130 - val_binary_accuracy: 0.9611\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 0s 206us/step - loss: 0.1002 - binary_accuracy: 0.9883 - val_loss: 0.1180 - val_binary_accuracy: 0.9722\n",
      "Epoch 18/200\n",
      "1448/1448 [==============================] - 0s 200us/step - loss: 0.0973 - binary_accuracy: 0.9869 - val_loss: 0.1209 - val_binary_accuracy: 0.9667\n",
      "Epoch 19/200\n",
      "1448/1448 [==============================] - 0s 189us/step - loss: 0.0924 - binary_accuracy: 0.9917 - val_loss: 0.1112 - val_binary_accuracy: 0.9611\n",
      "Epoch 20/200\n",
      "1448/1448 [==============================] - 0s 198us/step - loss: 0.0923 - binary_accuracy: 0.9869 - val_loss: 0.1417 - val_binary_accuracy: 0.9722\n",
      "Epoch 21/200\n",
      "1448/1448 [==============================] - 0s 217us/step - loss: 0.0873 - binary_accuracy: 0.9952 - val_loss: 0.1217 - val_binary_accuracy: 0.9667\n",
      "Epoch 22/200\n",
      "1448/1448 [==============================] - 0s 194us/step - loss: 0.0927 - binary_accuracy: 0.9862 - val_loss: 0.1265 - val_binary_accuracy: 0.9722\n",
      "Epoch 23/200\n",
      "1448/1448 [==============================] - 0s 198us/step - loss: 0.0903 - binary_accuracy: 0.9876 - val_loss: 0.1192 - val_binary_accuracy: 0.9722\n",
      "Epoch 24/200\n",
      "1448/1448 [==============================] - 0s 207us/step - loss: 0.0820 - binary_accuracy: 0.9924 - val_loss: 0.1383 - val_binary_accuracy: 0.9722\n",
      "Epoch 25/200\n",
      "1448/1448 [==============================] - 0s 198us/step - loss: 0.0815 - binary_accuracy: 0.9931 - val_loss: 0.1128 - val_binary_accuracy: 0.9667\n",
      "Epoch 26/200\n",
      "1448/1448 [==============================] - 0s 192us/step - loss: 0.0803 - binary_accuracy: 0.9938 - val_loss: 0.1211 - val_binary_accuracy: 0.9500\n",
      "Epoch 27/200\n",
      "1448/1448 [==============================] - 0s 198us/step - loss: 0.0799 - binary_accuracy: 0.9945 - val_loss: 0.1081 - val_binary_accuracy: 0.9611\n",
      "Epoch 28/200\n",
      "1448/1448 [==============================] - 0s 194us/step - loss: 0.0862 - binary_accuracy: 0.9924 - val_loss: 0.1149 - val_binary_accuracy: 0.9611\n",
      "Epoch 29/200\n",
      "1448/1448 [==============================] - 0s 198us/step - loss: 0.0861 - binary_accuracy: 0.9869 - val_loss: 0.1297 - val_binary_accuracy: 0.9667\n",
      "Epoch 30/200\n",
      "1448/1448 [==============================] - 0s 194us/step - loss: 0.0834 - binary_accuracy: 0.9890 - val_loss: 0.1321 - val_binary_accuracy: 0.9611\n",
      "Epoch 31/200\n",
      "1448/1448 [==============================] - 0s 192us/step - loss: 0.0743 - binary_accuracy: 0.9952 - val_loss: 0.1134 - val_binary_accuracy: 0.9722\n",
      "Epoch 32/200\n",
      "1448/1448 [==============================] - 0s 226us/step - loss: 0.0749 - binary_accuracy: 0.9945 - val_loss: 0.1809 - val_binary_accuracy: 0.9444\n",
      "Epoch 33/200\n",
      "1448/1448 [==============================] - 0s 189us/step - loss: 0.0830 - binary_accuracy: 0.9910 - val_loss: 0.1068 - val_binary_accuracy: 0.9778\n",
      "Epoch 34/200\n",
      "1448/1448 [==============================] - 0s 197us/step - loss: 0.0769 - binary_accuracy: 0.9945 - val_loss: 0.0955 - val_binary_accuracy: 0.9833\n",
      "Epoch 35/200\n",
      "1448/1448 [==============================] - 0s 197us/step - loss: 0.0731 - binary_accuracy: 0.9938 - val_loss: 0.1481 - val_binary_accuracy: 0.9667\n",
      "Epoch 36/200\n",
      "1448/1448 [==============================] - 0s 193us/step - loss: 0.0827 - binary_accuracy: 0.9883 - val_loss: 0.1263 - val_binary_accuracy: 0.9667\n",
      "Epoch 37/200\n",
      "1448/1448 [==============================] - 0s 191us/step - loss: 0.0785 - binary_accuracy: 0.9931 - val_loss: 0.0985 - val_binary_accuracy: 0.9778\n",
      "Epoch 38/200\n",
      "1448/1448 [==============================] - 0s 195us/step - loss: 0.0741 - binary_accuracy: 0.9924 - val_loss: 0.1157 - val_binary_accuracy: 0.9556\n",
      "Epoch 39/200\n",
      "1448/1448 [==============================] - 0s 199us/step - loss: 0.0800 - binary_accuracy: 0.9890 - val_loss: 0.1367 - val_binary_accuracy: 0.9667\n",
      "Epoch 40/200\n",
      "1448/1448 [==============================] - 0s 196us/step - loss: 0.0806 - binary_accuracy: 0.9917 - val_loss: 0.1694 - val_binary_accuracy: 0.9556\n",
      "Epoch 41/200\n",
      "1448/1448 [==============================] - 0s 197us/step - loss: 0.0792 - binary_accuracy: 0.9924 - val_loss: 0.1280 - val_binary_accuracy: 0.9833\n",
      "Epoch 42/200\n",
      "1448/1448 [==============================] - 0s 204us/step - loss: 0.0693 - binary_accuracy: 0.9952 - val_loss: 0.0995 - val_binary_accuracy: 0.9833\n",
      "Epoch 43/200\n",
      "1448/1448 [==============================] - 0s 245us/step - loss: 0.0661 - binary_accuracy: 0.9959 - val_loss: 0.1280 - val_binary_accuracy: 0.9611\n",
      "Epoch 44/200\n",
      "1448/1448 [==============================] - 0s 200us/step - loss: 0.0726 - binary_accuracy: 0.9917 - val_loss: 0.1534 - val_binary_accuracy: 0.9611\n",
      "1448/1448 [==============================] - 1s 409us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "180/180 [==============================] - 0s 83us/step\n",
      "(180,)\n",
      "(180,)\n",
      "182/182 [==============================] - 0s 82us/step\n",
      "(182,)\n",
      "(182,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 41, 16)            400       \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 656)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                10512     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 11,065\n",
      "Trainable params: 11,049\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1448 samples, validate on 182 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 2s 2ms/step - loss: 0.6099 - binary_accuracy: 0.6920 - val_loss: 0.4467 - val_binary_accuracy: 0.8352\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 200us/step - loss: 0.4096 - binary_accuracy: 0.8453 - val_loss: 0.3803 - val_binary_accuracy: 0.8626\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 196us/step - loss: 0.3407 - binary_accuracy: 0.8847 - val_loss: 0.3271 - val_binary_accuracy: 0.8736\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 253us/step - loss: 0.2938 - binary_accuracy: 0.9061 - val_loss: 0.3031 - val_binary_accuracy: 0.8901\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 214us/step - loss: 0.2506 - binary_accuracy: 0.9247 - val_loss: 0.2898 - val_binary_accuracy: 0.9121\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 216us/step - loss: 0.2228 - binary_accuracy: 0.9427 - val_loss: 0.2842 - val_binary_accuracy: 0.9231\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 192us/step - loss: 0.2061 - binary_accuracy: 0.9468 - val_loss: 0.2932 - val_binary_accuracy: 0.9341\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 195us/step - loss: 0.1810 - binary_accuracy: 0.9572 - val_loss: 0.2969 - val_binary_accuracy: 0.9396\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 192us/step - loss: 0.1746 - binary_accuracy: 0.9606 - val_loss: 0.2859 - val_binary_accuracy: 0.9176\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 262us/step - loss: 0.1440 - binary_accuracy: 0.9717 - val_loss: 0.2933 - val_binary_accuracy: 0.9121\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 260us/step - loss: 0.1289 - binary_accuracy: 0.9800 - val_loss: 0.3369 - val_binary_accuracy: 0.9451\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 232us/step - loss: 0.1330 - binary_accuracy: 0.9772 - val_loss: 0.3507 - val_binary_accuracy: 0.9396\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 194us/step - loss: 0.1240 - binary_accuracy: 0.9772 - val_loss: 0.3132 - val_binary_accuracy: 0.9231\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 224us/step - loss: 0.1189 - binary_accuracy: 0.9827 - val_loss: 0.3480 - val_binary_accuracy: 0.9341\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 257us/step - loss: 0.1127 - binary_accuracy: 0.9827 - val_loss: 0.3357 - val_binary_accuracy: 0.9286\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 217us/step - loss: 0.1076 - binary_accuracy: 0.9862 - val_loss: 0.3288 - val_binary_accuracy: 0.9176\n",
      "1448/1448 [==============================] - 1s 380us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "182/182 [==============================] - 0s 71us/step\n",
      "(182,)\n",
      "(182,)\n",
      "180/180 [==============================] - 0s 44us/step\n",
      "(180,)\n",
      "(180,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 41, 16)            400       \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 656)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                10512     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 11,065\n",
      "Trainable params: 11,049\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1448 samples, validate on 180 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 3s 2ms/step - loss: 0.6372 - binary_accuracy: 0.6609 - val_loss: 0.4886 - val_binary_accuracy: 0.8389\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 222us/step - loss: 0.4185 - binary_accuracy: 0.8474 - val_loss: 0.3954 - val_binary_accuracy: 0.8778\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 241us/step - loss: 0.3390 - binary_accuracy: 0.8847 - val_loss: 0.3577 - val_binary_accuracy: 0.8778\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 219us/step - loss: 0.2692 - binary_accuracy: 0.9137 - val_loss: 0.3342 - val_binary_accuracy: 0.8944\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 207us/step - loss: 0.2439 - binary_accuracy: 0.9261 - val_loss: 0.3316 - val_binary_accuracy: 0.9000\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 238us/step - loss: 0.2219 - binary_accuracy: 0.9392 - val_loss: 0.3227 - val_binary_accuracy: 0.8944\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 219us/step - loss: 0.1912 - binary_accuracy: 0.9558 - val_loss: 0.3263 - val_binary_accuracy: 0.9111\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 212us/step - loss: 0.1902 - binary_accuracy: 0.9530 - val_loss: 0.3111 - val_binary_accuracy: 0.9056\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 227us/step - loss: 0.1647 - binary_accuracy: 0.9710 - val_loss: 0.3108 - val_binary_accuracy: 0.9111\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 207us/step - loss: 0.1549 - binary_accuracy: 0.9703 - val_loss: 0.3047 - val_binary_accuracy: 0.9111\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 214us/step - loss: 0.1437 - binary_accuracy: 0.9738 - val_loss: 0.3041 - val_binary_accuracy: 0.9167\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 218us/step - loss: 0.1310 - binary_accuracy: 0.9814 - val_loss: 0.2880 - val_binary_accuracy: 0.9333\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 240us/step - loss: 0.1296 - binary_accuracy: 0.9786 - val_loss: 0.2817 - val_binary_accuracy: 0.9278\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 236us/step - loss: 0.1233 - binary_accuracy: 0.9793 - val_loss: 0.2939 - val_binary_accuracy: 0.9222\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 226us/step - loss: 0.1132 - binary_accuracy: 0.9890 - val_loss: 0.3230 - val_binary_accuracy: 0.9333\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 230us/step - loss: 0.1164 - binary_accuracy: 0.9883 - val_loss: 0.3082 - val_binary_accuracy: 0.9167\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 0s 227us/step - loss: 0.1092 - binary_accuracy: 0.9862 - val_loss: 0.3433 - val_binary_accuracy: 0.9111\n",
      "Epoch 18/200\n",
      "1448/1448 [==============================] - 0s 211us/step - loss: 0.1156 - binary_accuracy: 0.9848 - val_loss: 0.3253 - val_binary_accuracy: 0.9278\n",
      "Epoch 19/200\n",
      "1448/1448 [==============================] - 0s 236us/step - loss: 0.0985 - binary_accuracy: 0.9910 - val_loss: 0.3324 - val_binary_accuracy: 0.9222\n",
      "Epoch 20/200\n",
      "1448/1448 [==============================] - 0s 218us/step - loss: 0.0935 - binary_accuracy: 0.9890 - val_loss: 0.3087 - val_binary_accuracy: 0.9278\n",
      "Epoch 21/200\n",
      "1448/1448 [==============================] - 0s 233us/step - loss: 0.0945 - binary_accuracy: 0.9945 - val_loss: 0.3140 - val_binary_accuracy: 0.9278\n",
      "Epoch 22/200\n",
      "1448/1448 [==============================] - 0s 254us/step - loss: 0.0925 - binary_accuracy: 0.9924 - val_loss: 0.3328 - val_binary_accuracy: 0.9222\n",
      "Epoch 23/200\n",
      "1448/1448 [==============================] - 0s 224us/step - loss: 0.0907 - binary_accuracy: 0.9917 - val_loss: 0.3301 - val_binary_accuracy: 0.9389\n",
      "1448/1448 [==============================] - 1s 416us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "180/180 [==============================] - 0s 50us/step\n",
      "(180,)\n",
      "(180,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 0s 71us/step\n",
      "(182,)\n",
      "(182,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 41, 16)            400       \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 656)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                10512     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 11,065\n",
      "Trainable params: 11,049\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1448 samples, validate on 182 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 3s 2ms/step - loss: 0.6650 - binary_accuracy: 0.6416 - val_loss: 0.5924 - val_binary_accuracy: 0.7143\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 214us/step - loss: 0.4556 - binary_accuracy: 0.8322 - val_loss: 0.4482 - val_binary_accuracy: 0.8187\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 223us/step - loss: 0.3331 - binary_accuracy: 0.8950 - val_loss: 0.3720 - val_binary_accuracy: 0.8462\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 219us/step - loss: 0.2877 - binary_accuracy: 0.9081 - val_loss: 0.3592 - val_binary_accuracy: 0.8681\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 213us/step - loss: 0.2376 - binary_accuracy: 0.9309 - val_loss: 0.3414 - val_binary_accuracy: 0.8791\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 211us/step - loss: 0.2095 - binary_accuracy: 0.9461 - val_loss: 0.3478 - val_binary_accuracy: 0.8956\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 211us/step - loss: 0.1816 - binary_accuracy: 0.9606 - val_loss: 0.3238 - val_binary_accuracy: 0.9066\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 220us/step - loss: 0.1769 - binary_accuracy: 0.9565 - val_loss: 0.3704 - val_binary_accuracy: 0.9066\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 242us/step - loss: 0.1546 - binary_accuracy: 0.9717 - val_loss: 0.3605 - val_binary_accuracy: 0.9121\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 212us/step - loss: 0.1341 - binary_accuracy: 0.9751 - val_loss: 0.3669 - val_binary_accuracy: 0.9066\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 1s 375us/step - loss: 0.1294 - binary_accuracy: 0.9738 - val_loss: 0.3475 - val_binary_accuracy: 0.9176\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 304us/step - loss: 0.1191 - binary_accuracy: 0.9807 - val_loss: 0.3896 - val_binary_accuracy: 0.9011\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 266us/step - loss: 0.1178 - binary_accuracy: 0.9786 - val_loss: 0.3822 - val_binary_accuracy: 0.9121\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 1s 366us/step - loss: 0.1015 - binary_accuracy: 0.9855 - val_loss: 0.3876 - val_binary_accuracy: 0.9066\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 209us/step - loss: 0.1119 - binary_accuracy: 0.9814 - val_loss: 0.4254 - val_binary_accuracy: 0.9066\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 215us/step - loss: 0.0924 - binary_accuracy: 0.9869 - val_loss: 0.4614 - val_binary_accuracy: 0.9066\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 0s 244us/step - loss: 0.0865 - binary_accuracy: 0.9903 - val_loss: 0.4479 - val_binary_accuracy: 0.9121\n",
      "1448/1448 [==============================] - 1s 479us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "182/182 [==============================] - 0s 60us/step\n",
      "(182,)\n",
      "(182,)\n",
      "180/180 [==============================] - 0s 66us/step\n",
      "(180,)\n",
      "(180,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 41, 16)            400       \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 656)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 16)                10512     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 11,065\n",
      "Trainable params: 11,049\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1448 samples, validate on 180 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 3s 2ms/step - loss: 0.6194 - binary_accuracy: 0.6892 - val_loss: 0.5588 - val_binary_accuracy: 0.7611\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 241us/step - loss: 0.3961 - binary_accuracy: 0.8501 - val_loss: 0.4953 - val_binary_accuracy: 0.7833\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 240us/step - loss: 0.3117 - binary_accuracy: 0.9006 - val_loss: 0.4968 - val_binary_accuracy: 0.8056\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 228us/step - loss: 0.2456 - binary_accuracy: 0.9316 - val_loss: 0.4342 - val_binary_accuracy: 0.8444\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 256us/step - loss: 0.2164 - binary_accuracy: 0.9406 - val_loss: 0.4984 - val_binary_accuracy: 0.8167\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 226us/step - loss: 0.1873 - binary_accuracy: 0.9523 - val_loss: 0.4151 - val_binary_accuracy: 0.8389\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 272us/step - loss: 0.1680 - binary_accuracy: 0.9648 - val_loss: 0.4970 - val_binary_accuracy: 0.8167\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 232us/step - loss: 0.1405 - binary_accuracy: 0.9710 - val_loss: 0.4779 - val_binary_accuracy: 0.8222\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 224us/step - loss: 0.1310 - binary_accuracy: 0.9786 - val_loss: 0.4504 - val_binary_accuracy: 0.8333\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 237us/step - loss: 0.1270 - binary_accuracy: 0.9738 - val_loss: 0.5448 - val_binary_accuracy: 0.8278\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 237us/step - loss: 0.1114 - binary_accuracy: 0.9848 - val_loss: 0.5424 - val_binary_accuracy: 0.8278\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 234us/step - loss: 0.1131 - binary_accuracy: 0.9841 - val_loss: 0.5941 - val_binary_accuracy: 0.8333\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 238us/step - loss: 0.1077 - binary_accuracy: 0.9862 - val_loss: 0.5074 - val_binary_accuracy: 0.8333\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 260us/step - loss: 0.0915 - binary_accuracy: 0.9924 - val_loss: 0.6470 - val_binary_accuracy: 0.8222\n",
      "Epoch 15/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1448/1448 [==============================] - 0s 223us/step - loss: 0.0916 - binary_accuracy: 0.9876 - val_loss: 0.4828 - val_binary_accuracy: 0.8333\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 218us/step - loss: 0.0943 - binary_accuracy: 0.9890 - val_loss: 0.5827 - val_binary_accuracy: 0.8167\n",
      "1448/1448 [==============================] - 1s 511us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "180/180 [==============================] - 0s 194us/step\n",
      "(180,)\n",
      "(180,)\n",
      "182/182 [==============================] - 0s 55us/step\n",
      "(182,)\n",
      "(182,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 41, 16)            400       \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 656)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                10512     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 11,065\n",
      "Trainable params: 11,049\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1448 samples, validate on 182 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 3s 2ms/step - loss: 0.6437 - binary_accuracy: 0.6768 - val_loss: 0.6111 - val_binary_accuracy: 0.7418\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 261us/step - loss: 0.3970 - binary_accuracy: 0.8633 - val_loss: 0.5900 - val_binary_accuracy: 0.7527\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 287us/step - loss: 0.2902 - binary_accuracy: 0.9095 - val_loss: 0.6918 - val_binary_accuracy: 0.7857\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 248us/step - loss: 0.2331 - binary_accuracy: 0.9372 - val_loss: 0.6656 - val_binary_accuracy: 0.8187\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 248us/step - loss: 0.2067 - binary_accuracy: 0.9468 - val_loss: 0.6570 - val_binary_accuracy: 0.8242\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 239us/step - loss: 0.1690 - binary_accuracy: 0.9648 - val_loss: 0.6800 - val_binary_accuracy: 0.8132\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 243us/step - loss: 0.1483 - binary_accuracy: 0.9703 - val_loss: 0.6929 - val_binary_accuracy: 0.8297\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 243us/step - loss: 0.1355 - binary_accuracy: 0.9738 - val_loss: 0.7843 - val_binary_accuracy: 0.8242\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 258us/step - loss: 0.1256 - binary_accuracy: 0.9841 - val_loss: 0.7723 - val_binary_accuracy: 0.8352\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 256us/step - loss: 0.1230 - binary_accuracy: 0.9814 - val_loss: 0.7269 - val_binary_accuracy: 0.8297\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 256us/step - loss: 0.1076 - binary_accuracy: 0.9848 - val_loss: 0.8655 - val_binary_accuracy: 0.8132\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 277us/step - loss: 0.0884 - binary_accuracy: 0.9917 - val_loss: 0.9191 - val_binary_accuracy: 0.8242\n",
      "1448/1448 [==============================] - 1s 618us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "182/182 [==============================] - 0s 77us/step\n",
      "(182,)\n",
      "(182,)\n",
      "180/180 [==============================] - 0s 83us/step\n",
      "(180,)\n",
      "(180,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 41, 16)            400       \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 656)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                10512     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 11,065\n",
      "Trainable params: 11,049\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1448 samples, validate on 180 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 3s 2ms/step - loss: 0.6227 - binary_accuracy: 0.6941 - val_loss: 0.4761 - val_binary_accuracy: 0.8444\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 264us/step - loss: 0.4115 - binary_accuracy: 0.8543 - val_loss: 0.3588 - val_binary_accuracy: 0.8333\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 320us/step - loss: 0.3044 - binary_accuracy: 0.9068 - val_loss: 0.2611 - val_binary_accuracy: 0.9389\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 269us/step - loss: 0.2709 - binary_accuracy: 0.9199 - val_loss: 0.1752 - val_binary_accuracy: 0.9833\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 1s 357us/step - loss: 0.2329 - binary_accuracy: 0.9323 - val_loss: 0.1955 - val_binary_accuracy: 0.9833\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 274us/step - loss: 0.1976 - binary_accuracy: 0.9517 - val_loss: 0.1371 - val_binary_accuracy: 0.9944\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 324us/step - loss: 0.1673 - binary_accuracy: 0.9669 - val_loss: 0.1348 - val_binary_accuracy: 0.9944\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 1s 456us/step - loss: 0.1529 - binary_accuracy: 0.9648 - val_loss: 0.1210 - val_binary_accuracy: 0.9944\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 301us/step - loss: 0.1377 - binary_accuracy: 0.9751 - val_loss: 0.1376 - val_binary_accuracy: 0.9944\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 283us/step - loss: 0.1297 - binary_accuracy: 0.9758 - val_loss: 0.1162 - val_binary_accuracy: 0.9944\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 287us/step - loss: 0.1176 - binary_accuracy: 0.9820 - val_loss: 0.1089 - val_binary_accuracy: 0.9944\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 287us/step - loss: 0.1106 - binary_accuracy: 0.9841 - val_loss: 0.0977 - val_binary_accuracy: 0.9944\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 289us/step - loss: 0.1070 - binary_accuracy: 0.9855 - val_loss: 0.0958 - val_binary_accuracy: 0.9944\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 283us/step - loss: 0.0997 - binary_accuracy: 0.9910 - val_loss: 0.0850 - val_binary_accuracy: 0.9944\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 285us/step - loss: 0.0954 - binary_accuracy: 0.9896 - val_loss: 0.1032 - val_binary_accuracy: 0.9944\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 1s 378us/step - loss: 0.0954 - binary_accuracy: 0.9855 - val_loss: 0.0924 - val_binary_accuracy: 0.9944\n",
      "Epoch 17/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1448/1448 [==============================] - 0s 282us/step - loss: 0.0986 - binary_accuracy: 0.9848 - val_loss: 0.0975 - val_binary_accuracy: 0.9944\n",
      "Epoch 18/200\n",
      "1448/1448 [==============================] - 0s 260us/step - loss: 0.0813 - binary_accuracy: 0.9965 - val_loss: 0.0808 - val_binary_accuracy: 0.9944\n",
      "Epoch 19/200\n",
      "1448/1448 [==============================] - 0s 281us/step - loss: 0.0833 - binary_accuracy: 0.9931 - val_loss: 0.0839 - val_binary_accuracy: 0.9944\n",
      "Epoch 20/200\n",
      "1448/1448 [==============================] - 0s 341us/step - loss: 0.0799 - binary_accuracy: 0.9952 - val_loss: 0.0900 - val_binary_accuracy: 0.9944\n",
      "Epoch 21/200\n",
      "1448/1448 [==============================] - 0s 296us/step - loss: 0.0813 - binary_accuracy: 0.9910 - val_loss: 0.0835 - val_binary_accuracy: 0.9944\n",
      "Epoch 22/200\n",
      "1448/1448 [==============================] - 0s 276us/step - loss: 0.0857 - binary_accuracy: 0.9876 - val_loss: 0.0950 - val_binary_accuracy: 0.9944\n",
      "Epoch 23/200\n",
      "1448/1448 [==============================] - 0s 268us/step - loss: 0.0851 - binary_accuracy: 0.9910 - val_loss: 0.0782 - val_binary_accuracy: 0.9944\n",
      "Epoch 24/200\n",
      "1448/1448 [==============================] - 0s 291us/step - loss: 0.0793 - binary_accuracy: 0.9924 - val_loss: 0.0761 - val_binary_accuracy: 0.9944\n",
      "Epoch 25/200\n",
      "1448/1448 [==============================] - 0s 263us/step - loss: 0.0791 - binary_accuracy: 0.9931 - val_loss: 0.0812 - val_binary_accuracy: 0.9944\n",
      "Epoch 26/200\n",
      "1448/1448 [==============================] - 0s 254us/step - loss: 0.0692 - binary_accuracy: 0.9972 - val_loss: 0.0799 - val_binary_accuracy: 0.9833\n",
      "Epoch 27/200\n",
      "1448/1448 [==============================] - 0s 270us/step - loss: 0.0778 - binary_accuracy: 0.9903 - val_loss: 0.0870 - val_binary_accuracy: 0.9944\n",
      "Epoch 28/200\n",
      "1448/1448 [==============================] - 0s 264us/step - loss: 0.0744 - binary_accuracy: 0.9938 - val_loss: 0.0708 - val_binary_accuracy: 0.9944\n",
      "Epoch 29/200\n",
      "1448/1448 [==============================] - 0s 251us/step - loss: 0.0749 - binary_accuracy: 0.9945 - val_loss: 0.0935 - val_binary_accuracy: 0.9944\n",
      "Epoch 30/200\n",
      "1448/1448 [==============================] - 0s 256us/step - loss: 0.0719 - binary_accuracy: 0.9931 - val_loss: 0.0906 - val_binary_accuracy: 0.9944\n",
      "Epoch 31/200\n",
      "1448/1448 [==============================] - 0s 258us/step - loss: 0.0697 - binary_accuracy: 0.9972 - val_loss: 0.0844 - val_binary_accuracy: 0.9944\n",
      "Epoch 32/200\n",
      "1448/1448 [==============================] - 0s 276us/step - loss: 0.0677 - binary_accuracy: 0.9972 - val_loss: 0.0857 - val_binary_accuracy: 0.9944\n",
      "Epoch 33/200\n",
      "1448/1448 [==============================] - 0s 274us/step - loss: 0.0692 - binary_accuracy: 0.9959 - val_loss: 0.0984 - val_binary_accuracy: 0.9944\n",
      "Epoch 34/200\n",
      "1448/1448 [==============================] - 0s 256us/step - loss: 0.0803 - binary_accuracy: 0.9903 - val_loss: 0.1691 - val_binary_accuracy: 0.9556\n",
      "Epoch 35/200\n",
      "1448/1448 [==============================] - 0s 253us/step - loss: 0.0855 - binary_accuracy: 0.9862 - val_loss: 0.0828 - val_binary_accuracy: 0.9944\n",
      "Epoch 36/200\n",
      "1448/1448 [==============================] - 0s 257us/step - loss: 0.0801 - binary_accuracy: 0.9896 - val_loss: 0.0852 - val_binary_accuracy: 0.9944\n",
      "Epoch 37/200\n",
      "1448/1448 [==============================] - 0s 243us/step - loss: 0.0844 - binary_accuracy: 0.9862 - val_loss: 0.0715 - val_binary_accuracy: 0.9944\n",
      "Epoch 38/200\n",
      "1448/1448 [==============================] - 0s 250us/step - loss: 0.0731 - binary_accuracy: 0.9952 - val_loss: 0.0781 - val_binary_accuracy: 0.9944\n",
      "1448/1448 [==============================] - 1s 659us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "180/180 [==============================] - 0s 155us/step\n",
      "(180,)\n",
      "(180,)\n",
      "182/182 [==============================] - 0s 77us/step\n",
      "(182,)\n",
      "(182,)\n"
     ]
    }
   ],
   "source": [
    "All_data = 'G.subterraneus.fasta.txt' \n",
    "OutputDir = 'C:/Users/NSCL/N4-methylctosine//outputs/'\n",
    "funciton(All_data, OutputDir, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
