{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Conv1D, Input,MaxPooling1D,Flatten,LeakyReLU,AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from group_norm import GroupNormalization\n",
    "import random\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from keras import regularizers\n",
    "from keras.metrics import binary_accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix,recall_score,matthews_corrcoef,roc_curve,roc_auc_score,auc,precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau,LearningRateScheduler\n",
    "import os, sys, copy, getopt, re, argparse\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import tensorflow as tf\n",
    "from keras import losses\n",
    "import pickle\n",
    "\n",
    "from scipy import interp\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "import keras\n",
    "from keras.initializers import RandomUniform\n",
    "import keras.backend as K\n",
    "from random import shuffle\n",
    "import itertools \n",
    "\n",
    "class CustomModelCheckpoint(keras.callbacks.Callback):\n",
    "    def __init__(self, model, path):\n",
    "        self.path = path\n",
    "        self.model_for_saving = model\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        loss = logs['val_loss']\n",
    "        print(\"\\nSaving model to : {}\".format(self.path.format(epoch=epoch, val_loss=loss)))\n",
    "        self.model_for_saving.save_weights(self.path.format(epoch=epoch, val_loss=loss), overwrite=True)\n",
    "\n",
    "def step_decay(epoch):\n",
    "    lrate=[0.001]*30+[0.0001]*170\n",
    "    print (lrate[epoch])\n",
    "    return lrate[epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(temp, OutputDir):\n",
    "\n",
    "    trainning_result, validation_result, testing_result = temp;\n",
    "\n",
    "    file = open(OutputDir + '/performance.txt', 'w')\n",
    "\n",
    "    index = 0\n",
    "    for x in [trainning_result, validation_result, testing_result]:\n",
    "\n",
    "\n",
    "        title = ''\n",
    "\n",
    "        if index == 0:\n",
    "            title = 'training_'\n",
    "        if index == 1:\n",
    "            title = 'validation_'\n",
    "        if index == 2:\n",
    "            title = 'testing_'\n",
    "\n",
    "        index += 1;\n",
    "\n",
    "        file.write(title +  'results\\n')\n",
    "\n",
    "\n",
    "        for j in ['sn', 'sp', 'acc', 'MCC','AUC', 'precision', 'F1', 'lossValue']: #,'pre_recall_curve'\n",
    "\n",
    "            total = []\n",
    "\n",
    "            for val in x:\n",
    "                total.append(val[j])\n",
    "            file.write(j + ' : mean : ' + str(np.mean(total)) + ' std : ' + str(np.std(total))  + '\\n')\n",
    "\n",
    "        file.write('\\n\\n______________________________\\n')\n",
    "    file.close();\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for x in [trainning_result, validation_result, testing_result]:\n",
    "\n",
    "        tprs = []\n",
    "        aucs = []\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        \n",
    "        i = 0\n",
    "\n",
    "        for val in x:\n",
    "            tpr = val['tpr']\n",
    "            fpr = val['fpr']\n",
    "            tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "            tprs[-1][0] = 0.0\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            plt.plot(fpr, tpr, lw=1, alpha=0.3,label='ROC fold %d (AUC = %0.2f)' % (i+1, roc_auc))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        print;\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Random', alpha=.8)\n",
    "\n",
    "        mean_tpr = np.mean(tprs, axis=0)\n",
    "        mean_tpr[-1] = 1.0\n",
    "        mean_auc = auc(mean_fpr, mean_tpr)\n",
    "        std_auc = np.std(aucs)\n",
    "        plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "                 label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "                 lw=2, alpha=.8)\n",
    "\n",
    "        std_tpr = np.std(tprs, axis=0)\n",
    "        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "        plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                         label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        title = ''\n",
    "\n",
    "        if index == 0:\n",
    "            title = 'training_'\n",
    "        if index == 1:\n",
    "            title = 'validation_'\n",
    "        if index == 2:\n",
    "            title = 'testing_'\n",
    "\n",
    "        plt.savefig( OutputDir + '/' + title +'ROC.png')\n",
    "        plt.close('all');\n",
    "        \n",
    "       #************************** Precision Recall Curve*********************************\n",
    "        i = 0\n",
    "        prs = []\n",
    "        pre_aucs = []\n",
    "        mean_recal= np.linspace(0, 1, 100)\n",
    "        for val in x:\n",
    "            pre = val['prec']\n",
    "            rec = val['reca']\n",
    "            prs.append(interp(mean_recal, rec, pre))\n",
    "            prs[-1][0] = 0.0\n",
    "            p_r_auc = auc(rec, pre)\n",
    "            pre_aucs.append(p_r_auc)\n",
    "            plt.plot(rec, pre, lw=1, alpha=0.3,label='PRC fold %d (AUC = %0.2f)' % (i+1, p_r_auc))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        #plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Random', alpha=.8)\n",
    "\n",
    "        mean_pre = np.mean(prs, axis=0)\n",
    "        #mean_pre[-1] = 1.0\n",
    "        mean_auc = auc(mean_recal, mean_pre)\n",
    "        std_auc = np.std(pre_aucs)\n",
    "        plt.plot(mean_recal, mean_pre, color='b',\n",
    "                 label=r'Mean PRC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "                 lw=2, alpha=.8)\n",
    "\n",
    "        std_pre = np.std(prs, axis=0)\n",
    "        pre_upper = np.minimum(mean_pre + std_pre, 1)\n",
    "        pre_lower = np.maximum(mean_pre - std_pre, 0)\n",
    "        plt.fill_between(mean_recal, pre_lower, pre_upper, color='grey', alpha=.2,\n",
    "                         label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision Recall curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        title = ''\n",
    "\n",
    "        if index == 0:\n",
    "            title = 'training_'\n",
    "        if index == 1:\n",
    "            title = 'validation_'\n",
    "        if index == 2:\n",
    "            title = 'testing_'\n",
    "        if index == 3:\n",
    "            title = 'ind_testing_'\n",
    "\n",
    "        plt.savefig( OutputDir + '/' + title +'Pre_R_C.png')\n",
    "        plt.close('all');\n",
    "\n",
    "\n",
    "        index += 1;\n",
    "\n",
    "def chunkIt(seq, num):\n",
    "    avg = len(seq) / float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "    \n",
    "    return out\n",
    "\n",
    "def calculate(sequence):\n",
    "\n",
    "    X = []\n",
    "    dictNum = {'A' : 0, 'T' : 0, 'C' : 0, 'G' : 0};\n",
    "\n",
    "    for i in range(len(sequence)):\n",
    "\n",
    "        if sequence[i] in dictNum.keys():\n",
    "            dictNum[sequence[i]] += 1;\n",
    "            X.append(dictNum[sequence[i]] / float(i + 1));\n",
    "\n",
    "    return np.array(X)\n",
    "\n",
    "#Dinucleotide encoding\n",
    "# k-mer function\n",
    "def seq2k_mer(seqs):\n",
    "    k=2\n",
    "    sq2=[]\n",
    "    for seq in seqs:\n",
    "        sq1=[]\n",
    "        for x in range(len(seq)-1):\n",
    "            sq1+=[seq[x:x+k]]\n",
    "        sq2+=[sq1]\n",
    "    return sq2\n",
    "def get_onehot(seq): \n",
    "    bases = ['AA', 'AC', 'AG', 'AT', 'AN', 'CA', 'CC', 'CG', 'CT', 'CN', 'GA', 'GC', 'GG', 'GT', \n",
    "             'GN', 'TA', 'TC', 'TG', 'TT', 'TN', 'NA','NC', 'NG', 'NT', 'NN']\n",
    "    X = np.zeros((len(seq)-1, len(bases)))\n",
    "    #for i, char in enumerate(seq):\n",
    "    #    X[i, bases.index(char)] = 1\n",
    "    for i in range(40):\n",
    "        X[i, bases.index(seq[i:i+2])]=1\n",
    "    return X\n",
    "\n",
    "def dataProcessing(seq,key):\n",
    "    print(len(seq), len(seq[1]))\n",
    "    bases = ['AA', 'AC', 'AG', 'AT', 'AN', 'CA', 'CC', 'CG', 'CT', 'CN', 'GA', 'GC', 'GG', 'GT', \n",
    "             'GN', 'TA', 'TC', 'TG', 'TT', 'TN', 'NA','NC', 'NG', 'NT', 'NN']\n",
    "    X = np.zeros((len(seq),len(seq[0]), len(bases)))\n",
    "    for l,s in enumerate(seq):\n",
    "        for i, char in enumerate(s):\n",
    "            if char in bases:\n",
    "                X[l,i, bases.index(char)] = 1\n",
    "                \n",
    "    if key == 1:\n",
    "        lbs = list(np.ones(len(X)))#\n",
    "    if key == 2:\n",
    "        lbs=list(np.zeros(len(X)))\n",
    "    y = np.array(lbs, dtype = np.int32);\n",
    "    return X, y\n",
    "    ''''bases = ['A', 'C', 'G', 'T']\n",
    "    X = np.zeros((len(seq),len(seq[0]), len(bases)))\n",
    "    for l,s in enumerate(seq):\n",
    "        for i, char in enumerate(s):\n",
    "            if char in bases:\n",
    "                X[l,i, bases.index(char)] = 1\n",
    "    chem_bases = {'A':[1,1,1], 'C':[0,1,0], 'G':[1,0,0,], 'T':[0,0,1]}\n",
    "    Z = np.zeros((len(seq),len(seq[0]), 3))\n",
    "    for l,s in enumerate(seq):\n",
    "        for i, char in enumerate(s):\n",
    "            if char in chem_bases:\n",
    "                Z[l][i]=(chem_bases[char])\n",
    "                \n",
    "    all_features=np.concatenate([X,Z],axis=2)\n",
    "    if key == 1:\n",
    "        lbs = list(np.ones(len(X)))#\n",
    "    if key == 2:\n",
    "        lbs=list(np.zeros(len(X)))\n",
    "    y = np.array(lbs, dtype = np.int32)\n",
    " \n",
    "    return X, y #(n, 34, 4), (n,)'''''\n",
    "\n",
    "def prepareData(path):\n",
    "    all_seq = []\n",
    "    for seq_record in SeqIO.parse(path, \"fasta\"):\n",
    "        all_seq.append(str(seq_record.seq))\n",
    "    pos_seq = []\n",
    "    neg_seq = []\n",
    "    for i in range(len(all_seq)):\n",
    "        if(i < (len(all_seq)/2)):\n",
    "            pos_seq.append(all_seq[i])\n",
    "        else:\n",
    "            neg_seq.append(all_seq[i])\n",
    "    a=1\n",
    "    b=2\n",
    "    \n",
    "    Positive_X, Positive_y = dataProcessing(seq2k_mer(pos_seq),a)\n",
    "    Negitive_X, Negitive_y = dataProcessing(seq2k_mer(neg_seq),b)\n",
    "\n",
    "    return Positive_X, Positive_y, Negitive_X, Negitive_y\n",
    "\n",
    "def shuffleData(X, y):\n",
    "    index = [i for i in range(len(X))]\n",
    "    random.shuffle(index)\n",
    "    X = X[index]\n",
    "    y = y[index]\n",
    "    return X, y;\n",
    "\n",
    "def getMode():\n",
    "\n",
    "    input_shape = (40,25)\n",
    "\n",
    "    inputs = Input(shape = input_shape)\n",
    "\n",
    "    convLayer = Conv1D(filters = 32, kernel_size = 5,activation = 'elu',input_shape = input_shape, kernel_regularizer = regularizers.l2(1e-4), bias_regularizer = regularizers.l2(1e-4))(inputs)\n",
    "    normalizationLayer = GroupNormalization(groups = 4,axis=-1)(convLayer)\n",
    "    #normalizationLayer = BatchNormalization()(convLayer);\n",
    "    poolingLayer = MaxPooling1D(pool_size = 4, strides=2)(normalizationLayer)\n",
    "    dropoutLayer0 = Dropout(0.25)(poolingLayer)\n",
    "    \n",
    "    convLayer2 = Conv1D(filters = 32, kernel_size = 5,activation = 'elu',kernel_regularizer = regularizers.l2(1e-4), bias_regularizer = regularizers.l2(1e-4))(dropoutLayer0)\n",
    "    normalizationLayer2 = GroupNormalization(groups = 4,axis=-1)(convLayer2)\n",
    "    #normalizationLayer = BatchNormalization()(convLayer);\n",
    "    poolingLayer2 = MaxPooling1D(pool_size = 4, strides=2)(normalizationLayer2)\n",
    "    dropoutLayer = Dropout(0.25)(poolingLayer2)\n",
    "    \n",
    "    flattenLayer = Flatten()(dropoutLayer)\n",
    "\n",
    "    denseLayer = Dense(32, activation = 'elu',kernel_regularizer = regularizers.l2(1e-4),bias_regularizer = regularizers.l2(1e-4))(flattenLayer)\n",
    "    outLayer = Dense(1, activation='sigmoid')(denseLayer)\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = outLayer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer= SGD(momentum = 0.95, lr = 0.005),metrics=[binary_accuracy])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def calculateScore(X, y, model):\n",
    "    \n",
    "    score = model.evaluate(X,y)\n",
    "    pred_y = model.predict(X)\n",
    "    accuracy = score[1]\n",
    "    \n",
    "\n",
    "    tempLabel = np.zeros(shape = y.shape, dtype=np.int32)\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if pred_y[i] < 0.5:\n",
    "            tempLabel[i] = 0;\n",
    "        else:\n",
    "            tempLabel[i] = 1;\n",
    "    confusion = confusion_matrix(y, tempLabel)\n",
    "    TN, FP, FN, TP = confusion.ravel()\n",
    "\n",
    "    sensitivity = recall_score(y, tempLabel)\n",
    "    specificity = TN / float(TN+FP)\n",
    "    MCC = matthews_corrcoef(y, tempLabel)\n",
    "\n",
    "    F1Score = (2 * TP) / float(2 * TP + FP + FN)\n",
    "    precision = TP / float(TP + FP)\n",
    "    recall = TP/float (TP+FN)\n",
    "    pred_y = pred_y.reshape((-1, ))\n",
    "\n",
    "    ROCArea = roc_auc_score(y, pred_y)\n",
    "    fpr, tpr, thresholds = roc_curve(y, pred_y)\n",
    "    lossValue = None;\n",
    "    \n",
    "    pre, rec, threshlds = precision_recall_curve(y, pred_y)\n",
    "    pre = np.fliplr([pre])[0]  #so the array is increasing (you won't get negative AUC)\n",
    "    rec = np.fliplr([rec])[0]  \n",
    "    AUC_prec_rec = np.trapz(rec,pre)\n",
    "    AUC_prec_rec = abs(AUC_prec_rec)\n",
    "    \n",
    "    print(y.shape)\n",
    "    print(pred_y.shape)\n",
    "\n",
    "    y_true = tf.convert_to_tensor(y, np.float32)\n",
    "    y_pred = tf.convert_to_tensor(pred_y, np.float32)\n",
    "\n",
    "    with tf.Session():\n",
    "        lossValue = losses.binary_crossentropy(y_true, y_pred).eval()  #,'pre_recall_curve':AUC_prec_rec\n",
    "    plt.show()\n",
    "    return {'sn' : sensitivity, 'sp' : specificity, 'acc' : accuracy, 'MCC' : MCC, 'AUC' : ROCArea,'precision' : precision, 'F1' : F1Score, 'fpr' : fpr, 'tpr' : tpr, 'thresholds' : thresholds, 'lossValue' : lossValue,'pre_recall_curve':AUC_prec_rec,'prec':pre,'reca':rec}\n",
    "def test_data_prepro(ind_test):\n",
    "    Positive_X, Positive_y, Negitive_X, Negitive_y = prepareData(ind_test)\n",
    "    test_X = np.concatenate((Positive_X,Negitive_X))\n",
    "    test_y = np.concatenate((Positive_y,Negitive_y))\n",
    "    return test_X, test_y\n",
    "def funciton(All_data, OutputDir, folds):\n",
    "\n",
    "    Positive_X, Positive_y, Negitive_X, Negitive_y = prepareData(All_data)\n",
    "    \n",
    "    random.shuffle(Positive_X)\n",
    "    random.shuffle(Negitive_X)\n",
    "\n",
    "    Positive_X_Slices = chunkIt(Positive_X, folds)\n",
    "    Positive_y_Slices = chunkIt(Positive_y, folds)\n",
    "\n",
    "    Negative_X_Slices = chunkIt(Negitive_X, folds)\n",
    "    Negative_y_Slices = chunkIt(Negitive_y, folds)\n",
    "\n",
    "    trainning_result = []\n",
    "    validation_result = []\n",
    "    testing_result = []\n",
    "    #ind_testing_result = []\n",
    "    \n",
    "    for test_index in range(folds):\n",
    "\n",
    "        test_X = np.concatenate((Positive_X_Slices[test_index],Negative_X_Slices[test_index]))\n",
    "        test_y = np.concatenate((Positive_y_Slices[test_index],Negative_y_Slices[test_index]))\n",
    "        \n",
    "        validation_index = (test_index+1) % folds;\n",
    "\n",
    "        valid_X = np.concatenate((Positive_X_Slices[validation_index],Negative_X_Slices[validation_index]))\n",
    "        valid_y = np.concatenate((Positive_y_Slices[validation_index],Negative_y_Slices[validation_index]))\n",
    "\n",
    "        start = 0;\n",
    "\n",
    "        for val in range(0, folds):\n",
    "            if val != test_index and val != validation_index:\n",
    "                start = val;\n",
    "                break;\n",
    "\n",
    "        train_X = np.concatenate((Positive_X_Slices[start],Negative_X_Slices[start]))\n",
    "        train_y = np.concatenate((Positive_y_Slices[start],Negative_y_Slices[start]))\n",
    "\n",
    "        for i in range(0, folds):\n",
    "            if i != test_index and i != validation_index and i != start:\n",
    "                tempX = np.concatenate((Positive_X_Slices[i],Negative_X_Slices[i]))\n",
    "                tempy = np.concatenate((Positive_y_Slices[i],Negative_y_Slices[i]))\n",
    "\n",
    "                \n",
    "                train_X = np.concatenate((train_X, tempX))\n",
    "                train_y = np.concatenate((train_y, tempy))\n",
    "\n",
    "    \n",
    "        test_X, test_y = shuffleData(test_X,test_y);\n",
    "        valid_X,valid_y = shuffleData(valid_X,valid_y)\n",
    "        train_X,train_y = shuffleData(train_X,train_y);\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'x_test',test_X)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'y_test',test_y)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'valid_X',valid_X)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'valid_y',valid_y)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'x_train',train_X)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'y_train',train_y)\n",
    "        \n",
    "        #ind_test = 'H_200.txt'\n",
    "        #X_test,y_test=test_data_prepro(ind_test)\n",
    "        #ind_test_X, ind_test_y = shuffleData(X_test,y_test)\n",
    "        #np.save('/home/waleed/pseu_RNA/outputs/chunk_folds/'+str(test_index)+'_'+'ind_test_y',ind_test_X)\n",
    "        #np.save('/home/waleed/pseu_RNA/outputs/chunk_folds/'+str(test_index)+'_'+'ind_test_y',ind_test_y)\n",
    "        ###test####\n",
    "        \n",
    "        model = getMode()\n",
    "        \n",
    "        result_folder = OutputDir\n",
    "        if not os.path.exists(result_folder):\n",
    "            os.makedirs(result_folder)\n",
    "        model_results_folder=result_folder\n",
    "        \n",
    "        #best_weights = model_results_folder + 'best_weights.h5'\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience= 10, restore_best_weights=True)\n",
    "        model_check = ModelCheckpoint(filepath = OutputDir + \"/model\" + str(test_index+1) +\".h5\",mode='min', monitor = 'val_loss', save_best_only=True)#, save_weights_only=True\n",
    "        reduct_L_rate = ReduceLROnPlateau(monitor='val_loss',factor=0.01, patience=10)\n",
    "        lrate = LearningRateScheduler(step_decay)\n",
    "        cbacks = [model_check,reduct_L_rate,early_stopping]\n",
    "        \n",
    "        history = model.fit(train_X, train_y, batch_size = 16, epochs =200,validation_data = (valid_X, valid_y),callbacks = cbacks);\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        \n",
    "        model=load_model('C:/Users/NSCL/N4-methylctosine//outputs//model'+str(test_index+1)+'.h5')\n",
    "        \n",
    "        trainning_result.append(calculateScore(train_X, train_y, model))\n",
    "        validation_result.append(calculateScore(valid_X, valid_y, model))\n",
    "        testing_result.append(calculateScore(test_X, test_y, model))\n",
    "        #ind_testing_result.append(calculateScore(ind_test_X, ind_test_y, model));\n",
    "\n",
    "    temp_dict = (trainning_result, validation_result, testing_result)\n",
    "    analyze(temp_dict, OutputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data = 'D.melanogaster.fasta.txt' \n",
    "OutputDir = 'C:/Users/NSCL/N4-methylctosine//outputs/'\n",
    "funciton(All_data, OutputDir, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Conv1D, Input,MaxPooling1D,Flatten,LeakyReLU,AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from group_norm import GroupNormalization\n",
    "import random\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from keras import regularizers\n",
    "from keras.metrics import binary_accuracy\n",
    "from sklearn.metrics import confusion_matrix,recall_score,matthews_corrcoef,roc_curve,roc_auc_score,auc,precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau,LearningRateScheduler\n",
    "import os, sys, copy, getopt, re, argparse\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import tensorflow as tf\n",
    "from keras import losses\n",
    "import pickle\n",
    "\n",
    "from scipy import interp\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "import keras\n",
    "from keras.initializers import RandomUniform\n",
    "import keras.backend as K\n",
    "from random import shuffle\n",
    "import itertools \n",
    "\n",
    "class CustomModelCheckpoint(keras.callbacks.Callback):\n",
    "    def __init__(self, model, path):\n",
    "        self.path = path\n",
    "        self.model_for_saving = model\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        loss = logs['val_loss']\n",
    "        print(\"\\nSaving model to : {}\".format(self.path.format(epoch=epoch, val_loss=loss)))\n",
    "        self.model_for_saving.save_weights(self.path.format(epoch=epoch, val_loss=loss), overwrite=True)\n",
    "\n",
    "def step_decay(epoch):\n",
    "    lrate=[0.001]*30+[0.0001]*170\n",
    "    print (lrate[epoch])\n",
    "    return lrate[epoch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(temp, OutputDir):\n",
    "\n",
    "    trainning_result, validation_result, testing_result = temp;\n",
    "\n",
    "    file = open(OutputDir + '/performance.txt', 'w')\n",
    "\n",
    "    index = 0\n",
    "    for x in [trainning_result, validation_result, testing_result]:\n",
    "\n",
    "\n",
    "        title = ''\n",
    "\n",
    "        if index == 0:\n",
    "            title = 'training_'\n",
    "        if index == 1:\n",
    "            title = 'validation_'\n",
    "        if index == 2:\n",
    "            title = 'testing_'\n",
    "\n",
    "        index += 1;\n",
    "\n",
    "        file.write(title +  'results\\n')\n",
    "\n",
    "\n",
    "        for j in ['sn', 'sp', 'acc', 'MCC','AUC', 'precision', 'F1', 'lossValue']: #,'pre_recall_curve'\n",
    "\n",
    "            total = []\n",
    "\n",
    "            for val in x:\n",
    "                total.append(val[j])\n",
    "            file.write(j + ' : mean : ' + str(np.mean(total)) + ' std : ' + str(np.std(total))  + '\\n')\n",
    "\n",
    "        file.write('\\n\\n______________________________\\n')\n",
    "    file.close();\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for x in [trainning_result, validation_result, testing_result]:\n",
    "\n",
    "        tprs = []\n",
    "        aucs = []\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        \n",
    "        i = 0\n",
    "\n",
    "        for val in x:\n",
    "            tpr = val['tpr']\n",
    "            fpr = val['fpr']\n",
    "            tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "            tprs[-1][0] = 0.0\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            plt.plot(fpr, tpr, lw=1, alpha=0.3,label='ROC fold %d (AUC = %0.2f)' % (i+1, roc_auc))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        print;\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Random', alpha=.8)\n",
    "\n",
    "        mean_tpr = np.mean(tprs, axis=0)\n",
    "        mean_tpr[-1] = 1.0\n",
    "        mean_auc = auc(mean_fpr, mean_tpr)\n",
    "        std_auc = np.std(aucs)\n",
    "        plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "                 label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "                 lw=2, alpha=.8)\n",
    "\n",
    "        std_tpr = np.std(tprs, axis=0)\n",
    "        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "        plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                         label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        title = ''\n",
    "\n",
    "        if index == 0:\n",
    "            title = 'training_'\n",
    "        if index == 1:\n",
    "            title = 'validation_'\n",
    "        if index == 2:\n",
    "            title = 'testing_'\n",
    "\n",
    "        plt.savefig( OutputDir + '/' + title +'ROC.png')\n",
    "        plt.close('all');\n",
    "        \n",
    "       #************************** Precision Recall Curve*********************************\n",
    "        i = 0\n",
    "        prs = []\n",
    "        pre_aucs = []\n",
    "        mean_recal= np.linspace(0, 1, 100)\n",
    "        for val in x:\n",
    "            pre = val['prec']\n",
    "            rec = val['reca']\n",
    "            prs.append(interp(mean_recal, rec, pre))\n",
    "            prs[-1][0] = 0.0\n",
    "            p_r_auc = auc(rec, pre)\n",
    "            pre_aucs.append(p_r_auc)\n",
    "            plt.plot(rec, pre, lw=1, alpha=0.3,label='PRC fold %d (AUC = %0.2f)' % (i+1, p_r_auc))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        #plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Random', alpha=.8)\n",
    "\n",
    "        mean_pre = np.mean(prs, axis=0)\n",
    "        #mean_pre[-1] = 1.0\n",
    "        mean_auc = auc(mean_recal, mean_pre)\n",
    "        std_auc = np.std(pre_aucs)\n",
    "        plt.plot(mean_recal, mean_pre, color='b',\n",
    "                 label=r'Mean PRC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "                 lw=2, alpha=.8)\n",
    "\n",
    "        std_pre = np.std(prs, axis=0)\n",
    "        pre_upper = np.minimum(mean_pre + std_pre, 1)\n",
    "        pre_lower = np.maximum(mean_pre - std_pre, 0)\n",
    "        plt.fill_between(mean_recal, pre_lower, pre_upper, color='grey', alpha=.2,\n",
    "                         label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision Recall curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        title = ''\n",
    "\n",
    "        if index == 0:\n",
    "            title = 'training_'\n",
    "        if index == 1:\n",
    "            title = 'validation_'\n",
    "        if index == 2:\n",
    "            title = 'testing_'\n",
    "        if index == 3:\n",
    "            title = 'ind_testing_'\n",
    "\n",
    "        plt.savefig( OutputDir + '/' + title +'Pre_R_C.png')\n",
    "        plt.close('all');\n",
    "\n",
    "\n",
    "        index += 1;\n",
    "\n",
    "def chunkIt(seq, num):\n",
    "    avg = len(seq) / float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "    \n",
    "    return out\n",
    "\n",
    "def calculate(sequence):\n",
    "\n",
    "    X = []\n",
    "    dictNum = {'A' : 0, 'T' : 0, 'C' : 0, 'G' : 0};\n",
    "\n",
    "    for i in range(len(sequence)):\n",
    "\n",
    "        if sequence[i] in dictNum.keys():\n",
    "            dictNum[sequence[i]] += 1;\n",
    "            X.append(dictNum[sequence[i]] / float(i + 1));\n",
    "\n",
    "    return np.array(X)\n",
    "\n",
    "#nucleotide chemical properties and frequency encoding\n",
    "def cal(c,cb,i):\n",
    "    bases ={'A':[1,1,1], 'C':[0,1,0], 'G':[1,0,0,], 'T':[0,0,1]}\n",
    "    p=[]\n",
    "    p=bases[c]\n",
    "    p.append(np.round(cb/float(i+1),2))\n",
    "    return(p)\n",
    "def calculate(s):\n",
    "    p=f=list()\n",
    "    cba=cbc=cbt=cbg=0\n",
    "    for i,c in enumerate(s):\n",
    "        if c=='A':\n",
    "            cba+=1\n",
    "            p=cal(c,cba,i)\n",
    "        elif c=='T':\n",
    "            cbt+=1\n",
    "            p=cal(c,cbt,i)\n",
    "        elif c=='C':\n",
    "            cbc+=1\n",
    "            p=cal(c,cbc,i)\n",
    "        elif c=='G':\n",
    "            cbg+=1\n",
    "            p=cal(c,cbg,i)\n",
    "        else:\n",
    "            p=[0,0,0,0]\n",
    "        f.append(p)\n",
    "    return(f)\n",
    "# k-mer function\n",
    "def seq2k_mer(seqs):\n",
    "    k=2\n",
    "    sq2=[]\n",
    "    for seq in seqs:\n",
    "        sq1=[]\n",
    "        for x in range(len(seq)-1):\n",
    "            sq1+=[seq[x:x+k]]\n",
    "        sq2+=[sq1]\n",
    "    return sq2\n",
    "\n",
    "def dataProcessing(seq,key):\n",
    "    #bases = ['A', 'C', 'G', 'T']\n",
    "    bases = ['AA', 'AT' ,'AC', 'AG', 'TA' ,'TT', 'TC' ,'TG', 'CA', 'CT', 'CC', 'CG', 'GA', 'GT' ,'GC', 'GG']\n",
    "    X = np.zeros((len(seq),len(seq[0]), len(bases)))\n",
    "    for l,s in enumerate(seq):\n",
    "        for i, char in enumerate(s):\n",
    "            if char in bases:\n",
    "                X[l,i, bases.index(char)] = 1\n",
    "    \n",
    "    if key == 1:\n",
    "        lbs = list(np.ones(len(X)))#\n",
    "    if key == 2:\n",
    "        lbs=list(np.zeros(len(X)))\n",
    "    y = np.array(lbs, dtype = np.int32)\n",
    " \n",
    "    return X, y #(n, 34, 4), (n,)\n",
    "\n",
    "def prepareData(path):\n",
    "    all_seq = []\n",
    "    for seq_record in SeqIO.parse(path, \"fasta\"):\n",
    "        all_seq.append(str(seq_record.seq))\n",
    "    pos_seq = []\n",
    "    neg_seq = []\n",
    "    for i in range(len(all_seq)):\n",
    "        if(i < (len(all_seq)/2)):\n",
    "            pos_seq.append(all_seq[i])\n",
    "        else:\n",
    "            neg_seq.append(all_seq[i])\n",
    "    a=1\n",
    "    b=2\n",
    "    \n",
    "    Positive_X, Positive_y = dataProcessing(seq2k_mer(pos_seq),a);\n",
    "    Negitive_X, Negitive_y = dataProcessing(seq2k_mer(neg_seq),b);\n",
    "\n",
    "    return Positive_X, Positive_y, Negitive_X, Negitive_y\n",
    "\n",
    "def shuffleData(X, y):\n",
    "    index = [i for i in range(len(X))]\n",
    "    random.shuffle(index)\n",
    "    X = X[index]\n",
    "    y = y[index]\n",
    "    return X, y;\n",
    "\n",
    "def getMode():\n",
    "\n",
    "    input_shape = (40,16)\n",
    "\n",
    "    inputs = Input(shape = input_shape)\n",
    "\n",
    "    convLayer = Conv1D(filters = 8, kernel_size = 3,padding='same',kernel_regularizer = regularizers.l2(1e-3),bias_regularizer = regularizers.l2(1e-4),activation = 'relu',input_shape = input_shape)(inputs);\n",
    "    #normalizationLayer = GroupNormalization(groups = 4,axis=-1)(convLayer)\n",
    "    normalizationLayer=BatchNormalization()(convLayer)\n",
    "    #poolingLayer = AveragePooling1D(pool_size = 2,padding='same')(normalizationLayer)\n",
    "    convLayer2 = Conv1D(filters = 8, kernel_size = 3,padding='same',kernel_regularizer = regularizers.l2(1e-3),bias_regularizer = regularizers.l2(1e-4),activation = 'relu',input_shape = input_shape)(normalizationLayer)\n",
    "\n",
    "    flattenLayer = Flatten()(convLayer2)\n",
    "    denseLayer = Dense(5, activation = 'relu',kernel_regularizer = regularizers.l2(1e-3),bias_regularizer = regularizers.l2(1e-4))(flattenLayer)\n",
    "    dropoutLayer = Dropout(0.3)(denseLayer)\n",
    "    outLayer = Dense(1, activation='sigmoid')(dropoutLayer)\n",
    "    model = Model(inputs = inputs, outputs = outLayer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer= Adam(lr=0.001), metrics=['accuracy']); #SGD(lr = 0.005, momentum=0.95)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def calculateScore(X, y, model):\n",
    "    \n",
    "    score = model.evaluate(X,y)\n",
    "    pred_y = model.predict(X)\n",
    "    print(score)\n",
    "\n",
    "    accuracy = score[1];\n",
    "\n",
    "    tempLabel = np.zeros(shape = y.shape, dtype=np.int32)\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if pred_y[i] < 0.5:\n",
    "            tempLabel[i] = 0;\n",
    "        else:\n",
    "            tempLabel[i] = 1;\n",
    "    confusion = confusion_matrix(y, tempLabel)\n",
    "    TN, FP, FN, TP = confusion.ravel()\n",
    "\n",
    "    sensitivity = recall_score(y, tempLabel)\n",
    "    specificity = TN / float(TN+FP)\n",
    "    MCC = matthews_corrcoef(y, tempLabel)\n",
    "\n",
    "    F1Score = (2 * TP) / float(2 * TP + FP + FN)\n",
    "    precision = TP / float(TP + FP)\n",
    "    recall = TP/float (TP+FN)\n",
    "    pred_y = pred_y.reshape((-1, ))\n",
    "\n",
    "    ROCArea = roc_auc_score(y, pred_y)\n",
    "    fpr, tpr, thresholds = roc_curve(y, pred_y)\n",
    "    lossValue = None;\n",
    "    \n",
    "    pre, rec, threshlds = precision_recall_curve(y, pred_y)\n",
    "    pre = np.fliplr([pre])[0]  #so the array is increasing (you won't get negative AUC)\n",
    "    rec = np.fliplr([rec])[0]  \n",
    "    AUC_prec_rec = np.trapz(rec,pre)\n",
    "    AUC_prec_rec = abs(AUC_prec_rec)\n",
    "    \n",
    "    print(y.shape)\n",
    "    print(pred_y.shape)\n",
    "\n",
    "    y_true = tf.convert_to_tensor(y, np.float32)\n",
    "    y_pred = tf.convert_to_tensor(pred_y, np.float32)\n",
    "\n",
    "    with tf.Session():\n",
    "        lossValue = losses.binary_crossentropy(y_true, y_pred).eval()  #,'pre_recall_curve':AUC_prec_rec\n",
    "    plt.show()\n",
    "    return {'sn' : sensitivity, 'sp' : specificity, 'acc' : accuracy, 'MCC' : MCC, 'AUC' : ROCArea,'precision' : precision, 'F1' : F1Score, 'fpr' : fpr, 'tpr' : tpr, 'thresholds' : thresholds, 'lossValue' : lossValue,'pre_recall_curve':AUC_prec_rec,'prec':pre,'reca':rec}\n",
    "def test_data_prepro(ind_test):\n",
    "    Positive_X, Positive_y, Negitive_X, Negitive_y = prepareData(ind_test)\n",
    "    test_X = np.concatenate((Positive_X,Negitive_X))\n",
    "    test_y = np.concatenate((Positive_y,Negitive_y))\n",
    "    return test_X, test_y\n",
    "def funciton(All_data, OutputDir, folds):\n",
    "\n",
    "    Positive_X, Positive_y, Negitive_X, Negitive_y = prepareData(All_data)\n",
    "    \n",
    "    random.shuffle(Positive_X);\n",
    "    random.shuffle(Negitive_X);\n",
    "\n",
    "    Positive_X_Slices = chunkIt(Positive_X, folds);\n",
    "    Positive_y_Slices = chunkIt(Positive_y, folds);\n",
    "\n",
    "    Negative_X_Slices = chunkIt(Negitive_X, folds);\n",
    "    Negative_y_Slices = chunkIt(Negitive_y, folds);\n",
    "\n",
    "    trainning_result = []\n",
    "    validation_result = []\n",
    "    testing_result = []\n",
    "    #ind_testing_result = []\n",
    "    \n",
    "    for test_index in range(folds):\n",
    "\n",
    "        test_X = np.concatenate((Positive_X_Slices[test_index],Negative_X_Slices[test_index]))\n",
    "        test_y = np.concatenate((Positive_y_Slices[test_index],Negative_y_Slices[test_index]))\n",
    "        \n",
    "        validation_index = (test_index+1) % folds;\n",
    "\n",
    "        valid_X = np.concatenate((Positive_X_Slices[validation_index],Negative_X_Slices[validation_index]))\n",
    "        valid_y = np.concatenate((Positive_y_Slices[validation_index],Negative_y_Slices[validation_index]))\n",
    "\n",
    "        start = 0;\n",
    "\n",
    "        for val in range(0, folds):\n",
    "            if val != test_index and val != validation_index:\n",
    "                start = val;\n",
    "                break;\n",
    "\n",
    "        train_X = np.concatenate((Positive_X_Slices[start],Negative_X_Slices[start]))\n",
    "        train_y = np.concatenate((Positive_y_Slices[start],Negative_y_Slices[start]))\n",
    "\n",
    "        for i in range(0, folds):\n",
    "            if i != test_index and i != validation_index and i != start:\n",
    "                tempX = np.concatenate((Positive_X_Slices[i],Negative_X_Slices[i]))\n",
    "                tempy = np.concatenate((Positive_y_Slices[i],Negative_y_Slices[i]))\n",
    "\n",
    "                \n",
    "                train_X = np.concatenate((train_X, tempX))\n",
    "                train_y = np.concatenate((train_y, tempy))\n",
    "\n",
    "    \n",
    "        test_X, test_y = shuffleData(test_X,test_y);\n",
    "        valid_X,valid_y = shuffleData(valid_X,valid_y)\n",
    "        train_X,train_y = shuffleData(train_X,train_y);\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'x_test',test_X)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'y_test',test_y)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'valid_X',valid_X)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'valid_y',valid_y)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'x_train',train_X)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'y_train',train_y)\n",
    "        \n",
    "        #ind_test = 'H_200.txt'\n",
    "        #X_test,y_test=test_data_prepro(ind_test)\n",
    "        #ind_test_X, ind_test_y = shuffleData(X_test,y_test)\n",
    "        #np.save('/home/waleed/pseu_RNA/outputs/chunk_folds/'+str(test_index)+'_'+'ind_test_y',ind_test_X)\n",
    "        #np.save('/home/waleed/pseu_RNA/outputs/chunk_folds/'+str(test_index)+'_'+'ind_test_y',ind_test_y)\n",
    "        ###test####\n",
    "        \n",
    "        model = getMode()\n",
    "        \n",
    "        result_folder = OutputDir\n",
    "        if not os.path.exists(result_folder):\n",
    "            os.makedirs(result_folder)\n",
    "        model_results_folder=result_folder\n",
    "        \n",
    "        #best_weights = model_results_folder + 'best_weights.h5'\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience= 20, restore_best_weights=True)\n",
    "        model_check = ModelCheckpoint(filepath = OutputDir + \"/model\" + str(test_index+1) +\".h5\",mode='max', monitor = 'val_acc', save_best_only=True)#, save_weights_only=True\n",
    "        reduct_L_rate = ReduceLROnPlateau(monitor='val_loss',factor=0.01, patience=10)\n",
    "        lrate = LearningRateScheduler(step_decay)\n",
    "        cbacks = [model_check,reduct_L_rate,early_stopping]\n",
    "        \n",
    "        history = model.fit(train_X, train_y, batch_size = 32, epochs =200,validation_data = (valid_X, valid_y),callbacks = cbacks);\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        \n",
    "        model=load_model('C:/Users/NSCL/N4-methylctosine//outputs//model'+str(test_index+1)+'.h5')\n",
    "        \n",
    "        trainning_result.append(calculateScore(train_X, train_y, model))\n",
    "        validation_result.append(calculateScore(valid_X, valid_y, model))\n",
    "        testing_result.append(calculateScore(test_X, test_y, model))\n",
    "        #ind_testing_result.append(calculateScore(ind_test_X, ind_test_y, model));\n",
    "\n",
    "    temp_dict = (trainning_result, validation_result, testing_result)\n",
    "    analyze(temp_dict, OutputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data = 'G.subterraneus.fasta.txt' \n",
    "OutputDir = 'C:/Users/NSCL/N4-methylctosine//outputs/'\n",
    "funciton(All_data, OutputDir, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
