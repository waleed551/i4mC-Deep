{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Conv1D, Input,MaxPooling1D,Flatten,LeakyReLU,AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "import random\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from keras import regularizers\n",
    "from keras.metrics import binary_accuracy\n",
    "from sklearn.metrics import confusion_matrix,recall_score,matthews_corrcoef,roc_curve,roc_auc_score,auc,precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau,LearningRateScheduler\n",
    "import os, sys, copy, getopt, re, argparse\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import tensorflow as tf\n",
    "from keras import losses\n",
    "import pickle\n",
    "\n",
    "from scipy import interp\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "import keras\n",
    "from keras.initializers import RandomUniform\n",
    "import keras.backend as K\n",
    "from random import shuffle\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(temp, OutputDir):\n",
    "\n",
    "    trainning_result, validation_result, testing_result = temp\n",
    "\n",
    "    file = open(OutputDir + '/performance.txt', 'w')\n",
    "\n",
    "    index = 0\n",
    "    for x in [trainning_result, validation_result, testing_result]:\n",
    "\n",
    "\n",
    "        title = ''\n",
    "\n",
    "        if index == 0:\n",
    "            title = 'training_'\n",
    "        if index == 1:\n",
    "            title = 'validation_'\n",
    "        if index == 2:\n",
    "            title = 'testing_'\n",
    "\n",
    "        index += 1;\n",
    "\n",
    "        file.write(title +  'results\\n')\n",
    "\n",
    "\n",
    "        for j in ['sn', 'sp', 'acc', 'MCC','AUC', 'precision', 'F1', 'lossValue']: \n",
    "\n",
    "            total = []\n",
    "\n",
    "            for val in x:\n",
    "                total.append(val[j])\n",
    "            file.write(j + ' : mean : ' + str(np.mean(total)) + ' std : ' + str(np.std(total))  + '\\n')\n",
    "\n",
    "        file.write('\\n\\n______________________________\\n')\n",
    "    file.close()\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for x in [trainning_result, validation_result, testing_result]:\n",
    "\n",
    "        tprs = []\n",
    "        aucs = []\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        \n",
    "        i = 0\n",
    "\n",
    "        for val in x:\n",
    "            tpr = val['tpr']\n",
    "            fpr = val['fpr']\n",
    "            tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "            tprs[-1][0] = 0.0\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            plt.plot(fpr, tpr, lw=1, alpha=0.3,label='ROC fold %d (AUC = %0.2f)' % (i+1, roc_auc))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Random', alpha=.8)\n",
    "\n",
    "        mean_tpr = np.mean(tprs, axis=0)\n",
    "        mean_tpr[-1] = 1.0\n",
    "        mean_auc = auc(mean_fpr, mean_tpr)\n",
    "        std_auc = np.std(aucs)\n",
    "        plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "                 label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "                 lw=2, alpha=.8)\n",
    "\n",
    "        std_tpr = np.std(tprs, axis=0)\n",
    "        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "        plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                         label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        title = ''\n",
    "\n",
    "        if index == 0:\n",
    "            title = 'training_'\n",
    "        if index == 1:\n",
    "            title = 'validation_'\n",
    "        if index == 2:\n",
    "            title = 'testing_'\n",
    "\n",
    "        plt.savefig( OutputDir + '/' + title +'ROC.png')\n",
    "        plt.close('all')\n",
    "        \n",
    "       #************************** Precision Recall Curve*********************************\n",
    "        i = 0\n",
    "        prs = []\n",
    "        pre_aucs = []\n",
    "        mean_recal= np.linspace(0, 1, 100)\n",
    "        for val in x:\n",
    "            pre = val['prec']\n",
    "            rec = val['reca']\n",
    "            prs.append(interp(mean_recal, rec, pre))\n",
    "            prs[-1][0] = 0.0\n",
    "            p_r_auc = auc(rec, pre)\n",
    "            pre_aucs.append(p_r_auc)\n",
    "            plt.plot(rec, pre, lw=1, alpha=0.3,label='PRC fold %d (AUC = %0.2f)' % (i+1, p_r_auc))\n",
    "\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        mean_pre = np.mean(prs, axis=0)\n",
    "        mean_auc = auc(mean_recal, mean_pre)\n",
    "        std_auc = np.std(pre_aucs)\n",
    "        plt.plot(mean_recal, mean_pre, color='b',\n",
    "                 label=r'Mean PRC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "                 lw=2, alpha=.8)\n",
    "\n",
    "        std_pre = np.std(prs, axis=0)\n",
    "        pre_upper = np.minimum(mean_pre + std_pre, 1)\n",
    "        pre_lower = np.maximum(mean_pre - std_pre, 0)\n",
    "        plt.fill_between(mean_recal, pre_lower, pre_upper, color='grey', alpha=.2,\n",
    "                         label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision Recall curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        title = ''\n",
    "\n",
    "        if index == 0:\n",
    "            title = 'training_'\n",
    "        if index == 1:\n",
    "            title = 'validation_'\n",
    "        if index == 2:\n",
    "            title = 'testing_'\n",
    "\n",
    "        plt.savefig( OutputDir + '/' + title +'Pre_R_C.png')\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "        index += 1\n",
    "\n",
    "def chunkIt(seq, num):\n",
    "    avg = len(seq) / float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "#nucleotide chemical properties and frequency encoding\n",
    "def cal(c,cb,i):\n",
    "    bases ={'A':[1,1,1], 'C':[0,1,0], 'G':[1,0,0,], 'T':[0,0,1]}\n",
    "    p=[]\n",
    "    p=bases[c]\n",
    "    p.append(np.round(cb/float(i+1),2))\n",
    "    return(p)\n",
    "def calculate(s):\n",
    "    p=f=list()\n",
    "    cba=cbc=cbt=cbg=0\n",
    "    for i,c in enumerate(s):\n",
    "        if c=='A':\n",
    "            cba+=1\n",
    "            p=cal(c,cba,i)\n",
    "        elif c=='T':\n",
    "            cbt+=1\n",
    "            p=cal(c,cbt,i)\n",
    "        elif c=='C':\n",
    "            cbc+=1\n",
    "            p=cal(c,cbc,i)\n",
    "        elif c=='G':\n",
    "            cbg+=1\n",
    "            p=cal(c,cbg,i)\n",
    "        else:\n",
    "            p=[0,0,0,0]\n",
    "        f.append(p)\n",
    "    return(f)\n",
    "\n",
    "def dataProcessing(seq,key):\n",
    "    X_chem = []\n",
    "    for a in range(len(seq)):\n",
    "        X_chem.append(calculate(seq[a]))\n",
    "    X_chem = np.array(X_chem)         \n",
    "\n",
    "    if key == 1:\n",
    "        lbs = list(np.ones(len(X_chem)))\n",
    "    if key == 2:\n",
    "        lbs=list(np.zeros(len(X_chem)))\n",
    "    y = np.array(lbs, dtype = np.int32)\n",
    " \n",
    "    return X_chem, y\n",
    "\n",
    "def prepareData(path):\n",
    "    all_seq = []\n",
    "    for seq_record in SeqIO.parse(path, \"fasta\"):\n",
    "        all_seq.append(str(seq_record.seq))\n",
    "    pos_seq = []\n",
    "    neg_seq = []\n",
    "    for i in range(len(all_seq)):\n",
    "        if(i < (len(all_seq)/2)):\n",
    "            pos_seq.append(all_seq[i])\n",
    "        else:\n",
    "            neg_seq.append(all_seq[i])\n",
    "    a=1\n",
    "    b=2\n",
    "    \n",
    "    Positive_X, Positive_y = dataProcessing(pos_seq,a)\n",
    "    Negitive_X, Negitive_y = dataProcessing(neg_seq,b)\n",
    "\n",
    "    return Positive_X, Positive_y, Negitive_X, Negitive_y\n",
    "\n",
    "def shuffleData(X, y):\n",
    "    index = [i for i in range(len(X))]\n",
    "    random.shuffle(index)\n",
    "    X = X[index]\n",
    "    y = y[index]\n",
    "    return X, y\n",
    "\n",
    "def getMode():\n",
    "\n",
    "    input_shape = (41,4)\n",
    "\n",
    "    inputs = Input(shape = input_shape)\n",
    "\n",
    "    convLayer = Conv1D(filters = 8, kernel_size = 3,padding='same',kernel_regularizer = regularizers.l2(1e-3),bias_regularizer = regularizers.l2(1e-4),activation = 'relu',input_shape = input_shape)(inputs);\n",
    "    normalizationLayer=BatchNormalization()(convLayer)\n",
    "    convLayer2 = Conv1D(filters = 8, kernel_size = 3,padding='same',kernel_regularizer = regularizers.l2(1e-3),bias_regularizer = regularizers.l2(1e-4),activation = 'relu',input_shape = input_shape)(normalizationLayer)\n",
    "    flattenLayer = Flatten()(convLayer2)\n",
    "    denseLayer = Dense(8, activation = 'relu',kernel_regularizer = regularizers.l2(1e-3),bias_regularizer = regularizers.l2(1e-4))(flattenLayer)\n",
    "    dropoutLayer = Dropout(0.3)(denseLayer)\n",
    "    outLayer = Dense(1, activation='sigmoid')(dropoutLayer)\n",
    "    model = Model(inputs = inputs, outputs = outLayer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer= Adam(lr=0.001), metrics=[binary_accuracy])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def calculateScore(X, y, model):\n",
    "    \n",
    "    score = model.evaluate(X,y)\n",
    "    pred_y = model.predict(X)\n",
    "\n",
    "    accuracy = score[1]\n",
    "\n",
    "    tempLabel = np.zeros(shape = y.shape, dtype=np.int32)\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if pred_y[i] < 0.5:\n",
    "            tempLabel[i] = 0\n",
    "        else:\n",
    "            tempLabel[i] = 1\n",
    "    confusion = confusion_matrix(y, tempLabel)\n",
    "    TN, FP, FN, TP = confusion.ravel()\n",
    "\n",
    "    sensitivity = recall_score(y, tempLabel)\n",
    "    specificity = TN / float(TN+FP)\n",
    "    MCC = matthews_corrcoef(y, tempLabel)\n",
    "\n",
    "    F1Score = (2 * TP) / float(2 * TP + FP + FN)\n",
    "    precision = TP / float(TP + FP)\n",
    "    recall = TP/float (TP+FN)\n",
    "    pred_y = pred_y.reshape((-1, ))\n",
    "\n",
    "    ROCArea = roc_auc_score(y, pred_y)\n",
    "    fpr, tpr, thresholds = roc_curve(y, pred_y)\n",
    "    lossValue = None\n",
    "    \n",
    "    pre, rec, threshlds = precision_recall_curve(y, pred_y)\n",
    "    pre = np.fliplr([pre])[0] \n",
    "    rec = np.fliplr([rec])[0]  \n",
    "    AUC_prec_rec = np.trapz(rec,pre)\n",
    "    AUC_prec_rec = abs(AUC_prec_rec)\n",
    "    \n",
    "    print(y.shape)\n",
    "    print(pred_y.shape)\n",
    "\n",
    "    y_true = tf.convert_to_tensor(y, np.float32)\n",
    "    y_pred = tf.convert_to_tensor(pred_y, np.float32)\n",
    "\n",
    "    with tf.Session():\n",
    "        lossValue = losses.binary_crossentropy(y_true, y_pred).eval()  \n",
    "    plt.show()\n",
    "    return {'sn' : sensitivity, 'sp' : specificity, 'acc' : accuracy, 'MCC' : MCC, 'AUC' : ROCArea,'precision' : precision, 'F1' : F1Score, 'fpr' : fpr, 'tpr' : tpr, 'thresholds' : thresholds, 'lossValue' : lossValue,'pre_recall_curve':AUC_prec_rec,'prec':pre,'reca':rec}\n",
    "def test_data_prepro(ind_test):\n",
    "    Positive_X, Positive_y, Negitive_X, Negitive_y = prepareData(ind_test)\n",
    "    test_X = np.concatenate((Positive_X,Negitive_X))\n",
    "    test_y = np.concatenate((Positive_y,Negitive_y))\n",
    "    return test_X, test_y\n",
    "def funciton(All_data, OutputDir, folds):\n",
    "\n",
    "    Positive_X, Positive_y, Negitive_X, Negitive_y = prepareData(All_data)\n",
    "    random.shuffle(Positive_X)\n",
    "    random.shuffle(Negitive_X)\n",
    "    Positive_X_Slices = chunkIt(Positive_X, folds)\n",
    "    Positive_y_Slices = chunkIt(Positive_y, folds)\n",
    "    Negative_X_Slices = chunkIt(Negitive_X, folds)\n",
    "    Negative_y_Slices = chunkIt(Negitive_y, folds)\n",
    "\n",
    "    trainning_result = []\n",
    "    validation_result = []\n",
    "    testing_result = []\n",
    "    for test_index in range(folds):\n",
    "\n",
    "        test_X = np.concatenate((Positive_X_Slices[test_index],Negative_X_Slices[test_index]))\n",
    "        test_y = np.concatenate((Positive_y_Slices[test_index],Negative_y_Slices[test_index]))\n",
    "        \n",
    "        validation_index = (test_index+1) % folds\n",
    "\n",
    "        valid_X = np.concatenate((Positive_X_Slices[validation_index],Negative_X_Slices[validation_index]))\n",
    "        valid_y = np.concatenate((Positive_y_Slices[validation_index],Negative_y_Slices[validation_index]))\n",
    "\n",
    "        start = 0\n",
    "\n",
    "        for val in range(0, folds):\n",
    "            if val != test_index and val != validation_index:\n",
    "                start = val\n",
    "                break\n",
    "\n",
    "        train_X = np.concatenate((Positive_X_Slices[start],Negative_X_Slices[start]))\n",
    "        train_y = np.concatenate((Positive_y_Slices[start],Negative_y_Slices[start]))\n",
    "\n",
    "        for i in range(0, folds):\n",
    "            if i != test_index and i != validation_index and i != start:\n",
    "                tempX = np.concatenate((Positive_X_Slices[i],Negative_X_Slices[i]))\n",
    "                tempy = np.concatenate((Positive_y_Slices[i],Negative_y_Slices[i]))\n",
    "\n",
    "                \n",
    "                train_X = np.concatenate((train_X, tempX))\n",
    "                train_y = np.concatenate((train_y, tempy))\n",
    "\n",
    "    \n",
    "        test_X, test_y = shuffleData(test_X,test_y)\n",
    "        valid_X,valid_y = shuffleData(valid_X,valid_y)\n",
    "        train_X,train_y = shuffleData(train_X,train_y)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'x_test',test_X)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'y_test',test_y)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'valid_X',valid_X)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'valid_y',valid_y)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'x_train',train_X)\n",
    "        np.save('C:/Users/NSCL/N4-methylctosine//outputs/chunk_folds/'+str(test_index)+'_'+'y_train',train_y)\n",
    "        \n",
    "        \n",
    "        model = getMode()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience= 10, restore_best_weights=True)\n",
    "        model_check = ModelCheckpoint(filepath = OutputDir + \"/model\" + str(test_index+1) +\".h5\",mode='min', monitor = 'val_loss', save_best_only=True)#, save_weights_only=True\n",
    "        reduct_L_rate = ReduceLROnPlateau(monitor='val_loss',factor=0.01, patience=10)\n",
    "        cbacks = [model_check,reduct_L_rate,early_stopping]\n",
    "        \n",
    "        history = model.fit(train_X, train_y, batch_size = 16, epochs =200,validation_data = (valid_X, valid_y),callbacks = cbacks);\n",
    "        model=load_model('C:/Users/NSCL/N4-methylctosine//outputs//model'+str(test_index+1)+'.h5')\n",
    "        \n",
    "        trainning_result.append(calculateScore(train_X, train_y, model))\n",
    "        validation_result.append(calculateScore(valid_X, valid_y, model))\n",
    "        testing_result.append(calculateScore(test_X, test_y, model))\n",
    "\n",
    "    temp_dict = (trainning_result, validation_result, testing_result)\n",
    "    analyze(temp_dict, OutputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 41, 8)             200       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 2632      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,977\n",
      "Trainable params: 2,961\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1448 samples, validate on 182 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NSCL\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1448/1448 [==============================] - 1s 929us/step - loss: 0.6715 - binary_accuracy: 0.6133 - val_loss: 0.5312 - val_binary_accuracy: 0.7363\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 144us/step - loss: 0.5015 - binary_accuracy: 0.7831 - val_loss: 0.3984 - val_binary_accuracy: 0.8681\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 141us/step - loss: 0.3929 - binary_accuracy: 0.8384 - val_loss: 0.3260 - val_binary_accuracy: 0.8516\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 147us/step - loss: 0.3436 - binary_accuracy: 0.8695 - val_loss: 0.2781 - val_binary_accuracy: 0.9231\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 150us/step - loss: 0.3149 - binary_accuracy: 0.8888 - val_loss: 0.2269 - val_binary_accuracy: 0.9451\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 144us/step - loss: 0.2846 - binary_accuracy: 0.8978 - val_loss: 0.2024 - val_binary_accuracy: 0.9505\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 147us/step - loss: 0.2753 - binary_accuracy: 0.8950 - val_loss: 0.1835 - val_binary_accuracy: 0.9505\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 144us/step - loss: 0.2473 - binary_accuracy: 0.9102 - val_loss: 0.1479 - val_binary_accuracy: 0.9890\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 143us/step - loss: 0.2303 - binary_accuracy: 0.9171 - val_loss: 0.1599 - val_binary_accuracy: 0.9780\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 141us/step - loss: 0.2203 - binary_accuracy: 0.9330 - val_loss: 0.1382 - val_binary_accuracy: 1.0000\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 153us/step - loss: 0.1985 - binary_accuracy: 0.9385 - val_loss: 0.1309 - val_binary_accuracy: 0.9945\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 144us/step - loss: 0.1983 - binary_accuracy: 0.9337 - val_loss: 0.1188 - val_binary_accuracy: 0.9945\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 154us/step - loss: 0.1851 - binary_accuracy: 0.9448 - val_loss: 0.1325 - val_binary_accuracy: 0.9835\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 180us/step - loss: 0.1669 - binary_accuracy: 0.9530 - val_loss: 0.1026 - val_binary_accuracy: 0.9945\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 159us/step - loss: 0.1702 - binary_accuracy: 0.9475 - val_loss: 0.1108 - val_binary_accuracy: 0.9835\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 154us/step - loss: 0.1608 - binary_accuracy: 0.9482 - val_loss: 0.1118 - val_binary_accuracy: 0.9835\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 0s 156us/step - loss: 0.1459 - binary_accuracy: 0.9530 - val_loss: 0.0930 - val_binary_accuracy: 0.9945\n",
      "Epoch 18/200\n",
      "1448/1448 [==============================] - 0s 154us/step - loss: 0.1477 - binary_accuracy: 0.9572 - val_loss: 0.1038 - val_binary_accuracy: 0.9835\n",
      "Epoch 19/200\n",
      "1448/1448 [==============================] - 0s 178us/step - loss: 0.1505 - binary_accuracy: 0.9599 - val_loss: 0.1001 - val_binary_accuracy: 0.9890\n",
      "Epoch 20/200\n",
      "1448/1448 [==============================] - 0s 177us/step - loss: 0.1393 - binary_accuracy: 0.9613 - val_loss: 0.0942 - val_binary_accuracy: 0.9780\n",
      "Epoch 21/200\n",
      "1448/1448 [==============================] - 0s 180us/step - loss: 0.1319 - binary_accuracy: 0.9620 - val_loss: 0.0930 - val_binary_accuracy: 0.9945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200\n",
      "1448/1448 [==============================] - 0s 193us/step - loss: 0.1313 - binary_accuracy: 0.9613 - val_loss: 0.1164 - val_binary_accuracy: 0.9725\n",
      "Epoch 23/200\n",
      "1448/1448 [==============================] - 0s 145us/step - loss: 0.1236 - binary_accuracy: 0.9655 - val_loss: 0.0899 - val_binary_accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "1448/1448 [==============================] - 0s 152us/step - loss: 0.1126 - binary_accuracy: 0.9696 - val_loss: 0.0877 - val_binary_accuracy: 0.9835\n",
      "Epoch 25/200\n",
      "1448/1448 [==============================] - 0s 152us/step - loss: 0.1106 - binary_accuracy: 0.9689 - val_loss: 0.0670 - val_binary_accuracy: 0.9945\n",
      "Epoch 26/200\n",
      "1448/1448 [==============================] - 0s 153us/step - loss: 0.1188 - binary_accuracy: 0.9593 - val_loss: 0.0805 - val_binary_accuracy: 0.9945\n",
      "Epoch 27/200\n",
      "1448/1448 [==============================] - 0s 167us/step - loss: 0.1142 - binary_accuracy: 0.9682 - val_loss: 0.1111 - val_binary_accuracy: 0.9780\n",
      "Epoch 28/200\n",
      "1448/1448 [==============================] - 0s 180us/step - loss: 0.1221 - binary_accuracy: 0.9641 - val_loss: 0.1119 - val_binary_accuracy: 0.9835\n",
      "Epoch 29/200\n",
      "1448/1448 [==============================] - 0s 141us/step - loss: 0.1078 - binary_accuracy: 0.9682 - val_loss: 0.0991 - val_binary_accuracy: 0.9890\n",
      "Epoch 30/200\n",
      "1448/1448 [==============================] - 0s 143us/step - loss: 0.1128 - binary_accuracy: 0.9669 - val_loss: 0.0759 - val_binary_accuracy: 0.9945\n",
      "Epoch 31/200\n",
      "1448/1448 [==============================] - 0s 142us/step - loss: 0.1124 - binary_accuracy: 0.9641 - val_loss: 0.0933 - val_binary_accuracy: 0.9780\n",
      "Epoch 32/200\n",
      "1448/1448 [==============================] - 0s 154us/step - loss: 0.1120 - binary_accuracy: 0.9682 - val_loss: 0.0914 - val_binary_accuracy: 0.9890\n",
      "Epoch 33/200\n",
      "1448/1448 [==============================] - 0s 152us/step - loss: 0.1065 - binary_accuracy: 0.9669 - val_loss: 0.0766 - val_binary_accuracy: 0.9945\n",
      "Epoch 34/200\n",
      "1448/1448 [==============================] - 0s 147us/step - loss: 0.1042 - binary_accuracy: 0.9696 - val_loss: 0.0787 - val_binary_accuracy: 0.9945\n",
      "Epoch 35/200\n",
      "1448/1448 [==============================] - 0s 137us/step - loss: 0.1020 - binary_accuracy: 0.9669 - val_loss: 0.1042 - val_binary_accuracy: 0.9670\n",
      "1448/1448 [==============================] - 0s 153us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "182/182 [==============================] - 0s 49us/step\n",
      "(182,)\n",
      "(182,)\n",
      "180/180 [==============================] - 0s 50us/step\n",
      "(180,)\n",
      "(180,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 41, 8)             200       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 2632      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,977\n",
      "Trainable params: 2,961\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1448 samples, validate on 180 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 1s 1ms/step - loss: 0.7113 - binary_accuracy: 0.5573 - val_loss: 0.6795 - val_binary_accuracy: 0.6278\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 154us/step - loss: 0.6389 - binary_accuracy: 0.6354 - val_loss: 0.5920 - val_binary_accuracy: 0.7500\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 159us/step - loss: 0.5248 - binary_accuracy: 0.7514 - val_loss: 0.4412 - val_binary_accuracy: 0.8389\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 152us/step - loss: 0.4411 - binary_accuracy: 0.8142 - val_loss: 0.3823 - val_binary_accuracy: 0.8444\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 156us/step - loss: 0.3896 - binary_accuracy: 0.8460 - val_loss: 0.3218 - val_binary_accuracy: 0.8889\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 171us/step - loss: 0.3590 - binary_accuracy: 0.8584 - val_loss: 0.2812 - val_binary_accuracy: 0.9167\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 167us/step - loss: 0.3084 - binary_accuracy: 0.8847 - val_loss: 0.2183 - val_binary_accuracy: 0.9444\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 162us/step - loss: 0.2778 - binary_accuracy: 0.9019 - val_loss: 0.1996 - val_binary_accuracy: 0.9556\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 151us/step - loss: 0.2761 - binary_accuracy: 0.9088 - val_loss: 0.1747 - val_binary_accuracy: 0.9722\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 154us/step - loss: 0.2489 - binary_accuracy: 0.9171 - val_loss: 0.1845 - val_binary_accuracy: 0.9667\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 156us/step - loss: 0.2344 - binary_accuracy: 0.9227 - val_loss: 0.1700 - val_binary_accuracy: 0.9500\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 183us/step - loss: 0.2194 - binary_accuracy: 0.9358 - val_loss: 0.1513 - val_binary_accuracy: 0.9833\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 154us/step - loss: 0.2160 - binary_accuracy: 0.9427 - val_loss: 0.1572 - val_binary_accuracy: 0.9611\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 179us/step - loss: 0.1949 - binary_accuracy: 0.9461 - val_loss: 0.1590 - val_binary_accuracy: 0.9667\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 156us/step - loss: 0.1964 - binary_accuracy: 0.9434 - val_loss: 0.1663 - val_binary_accuracy: 0.9611\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 161us/step - loss: 0.1913 - binary_accuracy: 0.9503 - val_loss: 0.1413 - val_binary_accuracy: 0.9722\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 0s 154us/step - loss: 0.1813 - binary_accuracy: 0.9530 - val_loss: 0.1753 - val_binary_accuracy: 0.9667\n",
      "Epoch 18/200\n",
      "1448/1448 [==============================] - 0s 145us/step - loss: 0.1700 - binary_accuracy: 0.9565 - val_loss: 0.1883 - val_binary_accuracy: 0.9444\n",
      "Epoch 19/200\n",
      "1448/1448 [==============================] - 0s 150us/step - loss: 0.1517 - binary_accuracy: 0.9620 - val_loss: 0.1477 - val_binary_accuracy: 0.9611\n",
      "Epoch 20/200\n",
      "1448/1448 [==============================] - 0s 149us/step - loss: 0.1624 - binary_accuracy: 0.9620 - val_loss: 0.1472 - val_binary_accuracy: 0.9611\n",
      "Epoch 21/200\n",
      "1448/1448 [==============================] - 0s 159us/step - loss: 0.1519 - binary_accuracy: 0.9579 - val_loss: 0.1228 - val_binary_accuracy: 0.9722\n",
      "Epoch 22/200\n",
      "1448/1448 [==============================] - 0s 144us/step - loss: 0.1450 - binary_accuracy: 0.9662 - val_loss: 0.1094 - val_binary_accuracy: 0.9667\n",
      "Epoch 23/200\n",
      "1448/1448 [==============================] - 0s 147us/step - loss: 0.1440 - binary_accuracy: 0.9641 - val_loss: 0.1432 - val_binary_accuracy: 0.9611\n",
      "Epoch 24/200\n",
      "1448/1448 [==============================] - 0s 148us/step - loss: 0.1413 - binary_accuracy: 0.9689 - val_loss: 0.1280 - val_binary_accuracy: 0.9667\n",
      "Epoch 25/200\n",
      "1448/1448 [==============================] - 0s 146us/step - loss: 0.1369 - binary_accuracy: 0.9738 - val_loss: 0.1149 - val_binary_accuracy: 0.9667\n",
      "Epoch 26/200\n",
      "1448/1448 [==============================] - 0s 175us/step - loss: 0.1246 - binary_accuracy: 0.9765 - val_loss: 0.1422 - val_binary_accuracy: 0.9611\n",
      "Epoch 27/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1448/1448 [==============================] - 0s 146us/step - loss: 0.1245 - binary_accuracy: 0.9765 - val_loss: 0.1241 - val_binary_accuracy: 0.9667\n",
      "Epoch 28/200\n",
      "1448/1448 [==============================] - 0s 150us/step - loss: 0.1333 - binary_accuracy: 0.9689 - val_loss: 0.1035 - val_binary_accuracy: 0.9778\n",
      "Epoch 29/200\n",
      "1448/1448 [==============================] - 0s 153us/step - loss: 0.1259 - binary_accuracy: 0.9675 - val_loss: 0.1037 - val_binary_accuracy: 0.9667\n",
      "Epoch 30/200\n",
      "1448/1448 [==============================] - 0s 160us/step - loss: 0.1214 - binary_accuracy: 0.9717 - val_loss: 0.0980 - val_binary_accuracy: 0.9833\n",
      "Epoch 31/200\n",
      "1448/1448 [==============================] - 0s 147us/step - loss: 0.1151 - binary_accuracy: 0.9758 - val_loss: 0.1237 - val_binary_accuracy: 0.9556\n",
      "Epoch 32/200\n",
      "1448/1448 [==============================] - 0s 178us/step - loss: 0.1224 - binary_accuracy: 0.9758 - val_loss: 0.1695 - val_binary_accuracy: 0.9556\n",
      "Epoch 33/200\n",
      "1448/1448 [==============================] - 0s 182us/step - loss: 0.1014 - binary_accuracy: 0.9834 - val_loss: 0.1313 - val_binary_accuracy: 0.9722\n",
      "Epoch 34/200\n",
      "1448/1448 [==============================] - 0s 180us/step - loss: 0.1169 - binary_accuracy: 0.9793 - val_loss: 0.1149 - val_binary_accuracy: 0.9667\n",
      "Epoch 35/200\n",
      "1448/1448 [==============================] - 0s 169us/step - loss: 0.1082 - binary_accuracy: 0.9820 - val_loss: 0.1094 - val_binary_accuracy: 0.9833\n",
      "Epoch 36/200\n",
      "1448/1448 [==============================] - 0s 150us/step - loss: 0.1135 - binary_accuracy: 0.9807 - val_loss: 0.1592 - val_binary_accuracy: 0.9556\n",
      "Epoch 37/200\n",
      "1448/1448 [==============================] - 0s 163us/step - loss: 0.1119 - binary_accuracy: 0.9786 - val_loss: 0.1345 - val_binary_accuracy: 0.9667\n",
      "Epoch 38/200\n",
      "1448/1448 [==============================] - 0s 153us/step - loss: 0.1077 - binary_accuracy: 0.9779 - val_loss: 0.0966 - val_binary_accuracy: 0.9833\n",
      "Epoch 39/200\n",
      "1448/1448 [==============================] - 0s 165us/step - loss: 0.1103 - binary_accuracy: 0.9820 - val_loss: 0.1272 - val_binary_accuracy: 0.9722\n",
      "Epoch 40/200\n",
      "1448/1448 [==============================] - 0s 161us/step - loss: 0.1029 - binary_accuracy: 0.9779 - val_loss: 0.1542 - val_binary_accuracy: 0.9778\n",
      "Epoch 41/200\n",
      "1448/1448 [==============================] - 0s 154us/step - loss: 0.1062 - binary_accuracy: 0.9744 - val_loss: 0.1210 - val_binary_accuracy: 0.9778\n",
      "Epoch 42/200\n",
      "1448/1448 [==============================] - 0s 147us/step - loss: 0.1041 - binary_accuracy: 0.9814 - val_loss: 0.1397 - val_binary_accuracy: 0.9778\n",
      "Epoch 43/200\n",
      "1448/1448 [==============================] - 0s 156us/step - loss: 0.0999 - binary_accuracy: 0.9841 - val_loss: 0.1031 - val_binary_accuracy: 0.9722\n",
      "Epoch 44/200\n",
      "1448/1448 [==============================] - 0s 147us/step - loss: 0.1066 - binary_accuracy: 0.9758 - val_loss: 0.1254 - val_binary_accuracy: 0.9722\n",
      "Epoch 45/200\n",
      "1448/1448 [==============================] - 0s 152us/step - loss: 0.1057 - binary_accuracy: 0.9820 - val_loss: 0.1438 - val_binary_accuracy: 0.9667\n",
      "Epoch 46/200\n",
      "1448/1448 [==============================] - 0s 150us/step - loss: 0.0994 - binary_accuracy: 0.9827 - val_loss: 0.1349 - val_binary_accuracy: 0.9667\n",
      "Epoch 47/200\n",
      "1448/1448 [==============================] - 0s 151us/step - loss: 0.0902 - binary_accuracy: 0.9890 - val_loss: 0.1036 - val_binary_accuracy: 0.9778\n",
      "Epoch 48/200\n",
      "1448/1448 [==============================] - 0s 147us/step - loss: 0.0967 - binary_accuracy: 0.9820 - val_loss: 0.1282 - val_binary_accuracy: 0.9833\n",
      "1448/1448 [==============================] - 0s 212us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "180/180 [==============================] - 0s 61us/step\n",
      "(180,)\n",
      "(180,)\n",
      "182/182 [==============================] - 0s 33us/step\n",
      "(182,)\n",
      "(182,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 41, 8)             200       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 2632      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,977\n",
      "Trainable params: 2,961\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1448 samples, validate on 182 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 2s 1ms/step - loss: 0.6851 - binary_accuracy: 0.6050 - val_loss: 0.6015 - val_binary_accuracy: 0.6978\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 160us/step - loss: 0.5545 - binary_accuracy: 0.7403 - val_loss: 0.4938 - val_binary_accuracy: 0.7802\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 159us/step - loss: 0.4516 - binary_accuracy: 0.8128 - val_loss: 0.4229 - val_binary_accuracy: 0.8462\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 154us/step - loss: 0.4010 - binary_accuracy: 0.8370 - val_loss: 0.3737 - val_binary_accuracy: 0.8956\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 176us/step - loss: 0.3417 - binary_accuracy: 0.8722 - val_loss: 0.3342 - val_binary_accuracy: 0.8956\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 174us/step - loss: 0.3176 - binary_accuracy: 0.8771 - val_loss: 0.3112 - val_binary_accuracy: 0.9286\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 168us/step - loss: 0.2847 - binary_accuracy: 0.9075 - val_loss: 0.3111 - val_binary_accuracy: 0.9286\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 156us/step - loss: 0.2782 - binary_accuracy: 0.9040 - val_loss: 0.3117 - val_binary_accuracy: 0.9286\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 160us/step - loss: 0.2499 - binary_accuracy: 0.9178 - val_loss: 0.2777 - val_binary_accuracy: 0.9451\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 156us/step - loss: 0.2271 - binary_accuracy: 0.9275 - val_loss: 0.2894 - val_binary_accuracy: 0.9451\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 158us/step - loss: 0.2349 - binary_accuracy: 0.9344 - val_loss: 0.3087 - val_binary_accuracy: 0.9231\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 150us/step - loss: 0.2193 - binary_accuracy: 0.9399 - val_loss: 0.2690 - val_binary_accuracy: 0.9505\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 161us/step - loss: 0.1983 - binary_accuracy: 0.9461 - val_loss: 0.2589 - val_binary_accuracy: 0.9560\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 155us/step - loss: 0.1928 - binary_accuracy: 0.9482 - val_loss: 0.2472 - val_binary_accuracy: 0.9560\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 158us/step - loss: 0.1711 - binary_accuracy: 0.9565 - val_loss: 0.2784 - val_binary_accuracy: 0.9341\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 168us/step - loss: 0.1693 - binary_accuracy: 0.9593 - val_loss: 0.2467 - val_binary_accuracy: 0.9615\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 0s 162us/step - loss: 0.1767 - binary_accuracy: 0.9496 - val_loss: 0.2743 - val_binary_accuracy: 0.9451\n",
      "Epoch 18/200\n",
      "1448/1448 [==============================] - 0s 184us/step - loss: 0.1601 - binary_accuracy: 0.9620 - val_loss: 0.2702 - val_binary_accuracy: 0.9560\n",
      "Epoch 19/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1448/1448 [==============================] - 0s 249us/step - loss: 0.1559 - binary_accuracy: 0.9655 - val_loss: 0.2918 - val_binary_accuracy: 0.9396\n",
      "Epoch 20/200\n",
      "1448/1448 [==============================] - 0s 162us/step - loss: 0.1474 - binary_accuracy: 0.9662 - val_loss: 0.3179 - val_binary_accuracy: 0.9451\n",
      "Epoch 21/200\n",
      "1448/1448 [==============================] - 0s 169us/step - loss: 0.1472 - binary_accuracy: 0.9655 - val_loss: 0.2456 - val_binary_accuracy: 0.9560\n",
      "Epoch 22/200\n",
      "1448/1448 [==============================] - 0s 152us/step - loss: 0.1375 - binary_accuracy: 0.9744 - val_loss: 0.2594 - val_binary_accuracy: 0.9615\n",
      "Epoch 23/200\n",
      "1448/1448 [==============================] - 0s 175us/step - loss: 0.1427 - binary_accuracy: 0.9710 - val_loss: 0.2774 - val_binary_accuracy: 0.9505\n",
      "Epoch 24/200\n",
      "1448/1448 [==============================] - 0s 198us/step - loss: 0.1296 - binary_accuracy: 0.9689 - val_loss: 0.3109 - val_binary_accuracy: 0.9505\n",
      "Epoch 25/200\n",
      "1448/1448 [==============================] - 0s 203us/step - loss: 0.1252 - binary_accuracy: 0.9786 - val_loss: 0.2646 - val_binary_accuracy: 0.9615\n",
      "Epoch 26/200\n",
      "1448/1448 [==============================] - 0s 187us/step - loss: 0.1241 - binary_accuracy: 0.9786 - val_loss: 0.2705 - val_binary_accuracy: 0.9560\n",
      "Epoch 27/200\n",
      "1448/1448 [==============================] - 0s 155us/step - loss: 0.1184 - binary_accuracy: 0.9779 - val_loss: 0.3111 - val_binary_accuracy: 0.9560\n",
      "Epoch 28/200\n",
      "1448/1448 [==============================] - 0s 156us/step - loss: 0.1214 - binary_accuracy: 0.9744 - val_loss: 0.2898 - val_binary_accuracy: 0.9615\n",
      "Epoch 29/200\n",
      "1448/1448 [==============================] - 0s 158us/step - loss: 0.1132 - binary_accuracy: 0.9793 - val_loss: 0.3009 - val_binary_accuracy: 0.9560\n",
      "Epoch 30/200\n",
      "1448/1448 [==============================] - 0s 158us/step - loss: 0.1124 - binary_accuracy: 0.9765 - val_loss: 0.3144 - val_binary_accuracy: 0.9615\n",
      "Epoch 31/200\n",
      "1448/1448 [==============================] - 0s 188us/step - loss: 0.1102 - binary_accuracy: 0.9814 - val_loss: 0.3195 - val_binary_accuracy: 0.9560\n",
      "1448/1448 [==============================] - 0s 247us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "182/182 [==============================] - 0s 55us/step\n",
      "(182,)\n",
      "(182,)\n",
      "180/180 [==============================] - 0s 61us/step\n",
      "(180,)\n",
      "(180,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 41, 8)             200       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 2632      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,977\n",
      "Trainable params: 2,961\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1448 samples, validate on 180 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 2s 1ms/step - loss: 0.6643 - binary_accuracy: 0.6188 - val_loss: 0.5410 - val_binary_accuracy: 0.7778\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 161us/step - loss: 0.4919 - binary_accuracy: 0.7721 - val_loss: 0.3534 - val_binary_accuracy: 0.9000\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 163us/step - loss: 0.4013 - binary_accuracy: 0.8467 - val_loss: 0.2938 - val_binary_accuracy: 0.9056\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 158us/step - loss: 0.3429 - binary_accuracy: 0.8667 - val_loss: 0.2810 - val_binary_accuracy: 0.8944\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 163us/step - loss: 0.3183 - binary_accuracy: 0.8847 - val_loss: 0.2432 - val_binary_accuracy: 0.9167\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 159us/step - loss: 0.2854 - binary_accuracy: 0.8992 - val_loss: 0.2353 - val_binary_accuracy: 0.9222\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 182us/step - loss: 0.2658 - binary_accuracy: 0.9095 - val_loss: 0.1999 - val_binary_accuracy: 0.9389\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 174us/step - loss: 0.2357 - binary_accuracy: 0.9178 - val_loss: 0.2023 - val_binary_accuracy: 0.9556\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 171us/step - loss: 0.2333 - binary_accuracy: 0.9220 - val_loss: 0.2165 - val_binary_accuracy: 0.9278\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 164us/step - loss: 0.2174 - binary_accuracy: 0.9282 - val_loss: 0.1866 - val_binary_accuracy: 0.9556\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 171us/step - loss: 0.1945 - binary_accuracy: 0.9385 - val_loss: 0.1955 - val_binary_accuracy: 0.9500\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 173us/step - loss: 0.1776 - binary_accuracy: 0.9454 - val_loss: 0.1797 - val_binary_accuracy: 0.9500\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 170us/step - loss: 0.1773 - binary_accuracy: 0.9489 - val_loss: 0.1837 - val_binary_accuracy: 0.9444\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 157us/step - loss: 0.1641 - binary_accuracy: 0.9454 - val_loss: 0.1641 - val_binary_accuracy: 0.9556\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 160us/step - loss: 0.1567 - binary_accuracy: 0.9530 - val_loss: 0.1627 - val_binary_accuracy: 0.9444\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 159us/step - loss: 0.1494 - binary_accuracy: 0.9648 - val_loss: 0.1642 - val_binary_accuracy: 0.9500\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 0s 176us/step - loss: 0.1542 - binary_accuracy: 0.9510 - val_loss: 0.1620 - val_binary_accuracy: 0.9556\n",
      "Epoch 18/200\n",
      "1448/1448 [==============================] - 0s 165us/step - loss: 0.1438 - binary_accuracy: 0.9586 - val_loss: 0.1607 - val_binary_accuracy: 0.9611\n",
      "Epoch 19/200\n",
      "1448/1448 [==============================] - 0s 165us/step - loss: 0.1308 - binary_accuracy: 0.9689 - val_loss: 0.1596 - val_binary_accuracy: 0.9611\n",
      "Epoch 20/200\n",
      "1448/1448 [==============================] - 0s 181us/step - loss: 0.1362 - binary_accuracy: 0.9593 - val_loss: 0.1543 - val_binary_accuracy: 0.9611\n",
      "Epoch 21/200\n",
      "1448/1448 [==============================] - 0s 168us/step - loss: 0.1204 - binary_accuracy: 0.9703 - val_loss: 0.1660 - val_binary_accuracy: 0.9611\n",
      "Epoch 22/200\n",
      "1448/1448 [==============================] - 0s 161us/step - loss: 0.1139 - binary_accuracy: 0.9751 - val_loss: 0.1761 - val_binary_accuracy: 0.9444\n",
      "Epoch 23/200\n",
      "1448/1448 [==============================] - 0s 166us/step - loss: 0.1168 - binary_accuracy: 0.9682 - val_loss: 0.1685 - val_binary_accuracy: 0.9444\n",
      "Epoch 24/200\n",
      "1448/1448 [==============================] - 0s 176us/step - loss: 0.1173 - binary_accuracy: 0.9758 - val_loss: 0.1698 - val_binary_accuracy: 0.9444\n",
      "Epoch 25/200\n",
      "1448/1448 [==============================] - 0s 205us/step - loss: 0.1060 - binary_accuracy: 0.9751 - val_loss: 0.1580 - val_binary_accuracy: 0.9556\n",
      "Epoch 26/200\n",
      "1448/1448 [==============================] - 0s 205us/step - loss: 0.1032 - binary_accuracy: 0.9786 - val_loss: 0.2089 - val_binary_accuracy: 0.9500\n",
      "Epoch 27/200\n",
      "1448/1448 [==============================] - 0s 183us/step - loss: 0.1137 - binary_accuracy: 0.9717 - val_loss: 0.1719 - val_binary_accuracy: 0.9556\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1448/1448 [==============================] - 0s 163us/step - loss: 0.1110 - binary_accuracy: 0.9717 - val_loss: 0.1826 - val_binary_accuracy: 0.9611\n",
      "Epoch 29/200\n",
      "1448/1448 [==============================] - 0s 163us/step - loss: 0.0917 - binary_accuracy: 0.9793 - val_loss: 0.1813 - val_binary_accuracy: 0.9500\n",
      "Epoch 30/200\n",
      "1448/1448 [==============================] - 0s 157us/step - loss: 0.1035 - binary_accuracy: 0.9765 - val_loss: 0.1896 - val_binary_accuracy: 0.9556\n",
      "1448/1448 [==============================] - 0s 309us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "180/180 [==============================] - 0s 55us/step\n",
      "(180,)\n",
      "(180,)\n",
      "182/182 [==============================] - 0s 49us/step\n",
      "(182,)\n",
      "(182,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 41, 8)             200       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 8)                 2632      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,977\n",
      "Trainable params: 2,961\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1448 samples, validate on 182 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 2s 2ms/step - loss: 0.6952 - binary_accuracy: 0.6001 - val_loss: 0.6102 - val_binary_accuracy: 0.7088\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 189us/step - loss: 0.5540 - binary_accuracy: 0.7203 - val_loss: 0.5401 - val_binary_accuracy: 0.7637\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 167us/step - loss: 0.4602 - binary_accuracy: 0.7935 - val_loss: 0.4861 - val_binary_accuracy: 0.7747\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 182us/step - loss: 0.4044 - binary_accuracy: 0.8329 - val_loss: 0.4672 - val_binary_accuracy: 0.7802\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 167us/step - loss: 0.3483 - binary_accuracy: 0.8660 - val_loss: 0.4367 - val_binary_accuracy: 0.7967\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 174us/step - loss: 0.3345 - binary_accuracy: 0.8819 - val_loss: 0.4076 - val_binary_accuracy: 0.8516\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 167us/step - loss: 0.2967 - binary_accuracy: 0.9047 - val_loss: 0.3984 - val_binary_accuracy: 0.8352\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 169us/step - loss: 0.2757 - binary_accuracy: 0.9061 - val_loss: 0.3857 - val_binary_accuracy: 0.8626\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 167us/step - loss: 0.2578 - binary_accuracy: 0.9151 - val_loss: 0.3860 - val_binary_accuracy: 0.8516\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 175us/step - loss: 0.2480 - binary_accuracy: 0.9164 - val_loss: 0.3736 - val_binary_accuracy: 0.8846\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 169us/step - loss: 0.2317 - binary_accuracy: 0.9296 - val_loss: 0.3415 - val_binary_accuracy: 0.8846\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 168us/step - loss: 0.2086 - binary_accuracy: 0.9392 - val_loss: 0.3601 - val_binary_accuracy: 0.8791\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 176us/step - loss: 0.1968 - binary_accuracy: 0.9399 - val_loss: 0.3524 - val_binary_accuracy: 0.8736\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 189us/step - loss: 0.1939 - binary_accuracy: 0.9434 - val_loss: 0.3234 - val_binary_accuracy: 0.8901\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 172us/step - loss: 0.1808 - binary_accuracy: 0.9475 - val_loss: 0.3321 - val_binary_accuracy: 0.8956\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 171us/step - loss: 0.1788 - binary_accuracy: 0.9434 - val_loss: 0.3301 - val_binary_accuracy: 0.9121\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 0s 169us/step - loss: 0.1669 - binary_accuracy: 0.9523 - val_loss: 0.3290 - val_binary_accuracy: 0.9011\n",
      "Epoch 18/200\n",
      "1448/1448 [==============================] - 0s 202us/step - loss: 0.1611 - binary_accuracy: 0.9523 - val_loss: 0.3376 - val_binary_accuracy: 0.9066\n",
      "Epoch 19/200\n",
      "1448/1448 [==============================] - 0s 211us/step - loss: 0.1439 - binary_accuracy: 0.9620 - val_loss: 0.3209 - val_binary_accuracy: 0.9121\n",
      "Epoch 20/200\n",
      "1448/1448 [==============================] - 0s 205us/step - loss: 0.1502 - binary_accuracy: 0.9634 - val_loss: 0.3387 - val_binary_accuracy: 0.9176\n",
      "Epoch 21/200\n",
      "1448/1448 [==============================] - 0s 169us/step - loss: 0.1479 - binary_accuracy: 0.9606 - val_loss: 0.3326 - val_binary_accuracy: 0.9231\n",
      "Epoch 22/200\n",
      "1448/1448 [==============================] - 0s 165us/step - loss: 0.1414 - binary_accuracy: 0.9530 - val_loss: 0.3946 - val_binary_accuracy: 0.9011\n",
      "Epoch 23/200\n",
      "1448/1448 [==============================] - 0s 176us/step - loss: 0.1374 - binary_accuracy: 0.9613 - val_loss: 0.3702 - val_binary_accuracy: 0.9121\n",
      "Epoch 24/200\n",
      "1448/1448 [==============================] - 0s 176us/step - loss: 0.1234 - binary_accuracy: 0.9675 - val_loss: 0.3592 - val_binary_accuracy: 0.9176\n",
      "Epoch 25/200\n",
      "1448/1448 [==============================] - 0s 183us/step - loss: 0.1202 - binary_accuracy: 0.9731 - val_loss: 0.3721 - val_binary_accuracy: 0.9011\n",
      "Epoch 26/200\n",
      "1448/1448 [==============================] - 0s 187us/step - loss: 0.1311 - binary_accuracy: 0.9641 - val_loss: 0.3849 - val_binary_accuracy: 0.9011\n",
      "Epoch 27/200\n",
      "1448/1448 [==============================] - 0s 178us/step - loss: 0.1209 - binary_accuracy: 0.9703 - val_loss: 0.3965 - val_binary_accuracy: 0.9066\n",
      "Epoch 28/200\n",
      "1448/1448 [==============================] - 0s 174us/step - loss: 0.1196 - binary_accuracy: 0.9751 - val_loss: 0.4016 - val_binary_accuracy: 0.9011\n",
      "Epoch 29/200\n",
      "1448/1448 [==============================] - 0s 178us/step - loss: 0.1133 - binary_accuracy: 0.9751 - val_loss: 0.4468 - val_binary_accuracy: 0.8956\n",
      "1448/1448 [==============================] - 1s 366us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "182/182 [==============================] - 0s 71us/step\n",
      "(182,)\n",
      "(182,)\n",
      "180/180 [==============================] - 0s 66us/step\n",
      "(180,)\n",
      "(180,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 41, 8)             200       \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 8)                 2632      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,977\n",
      "Trainable params: 2,961\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1448 samples, validate on 180 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 2s 2ms/step - loss: 0.6748 - binary_accuracy: 0.6236 - val_loss: 0.5838 - val_binary_accuracy: 0.7278\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 176us/step - loss: 0.5104 - binary_accuracy: 0.7776 - val_loss: 0.4870 - val_binary_accuracy: 0.7611\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 185us/step - loss: 0.4249 - binary_accuracy: 0.8398 - val_loss: 0.4088 - val_binary_accuracy: 0.8222\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 200us/step - loss: 0.3515 - binary_accuracy: 0.8695 - val_loss: 0.3835 - val_binary_accuracy: 0.8556\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 174us/step - loss: 0.3188 - binary_accuracy: 0.8909 - val_loss: 0.3771 - val_binary_accuracy: 0.8278\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 177us/step - loss: 0.2897 - binary_accuracy: 0.9047 - val_loss: 0.3771 - val_binary_accuracy: 0.8389\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 184us/step - loss: 0.2620 - binary_accuracy: 0.9130 - val_loss: 0.3409 - val_binary_accuracy: 0.8500\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 227us/step - loss: 0.2530 - binary_accuracy: 0.9185 - val_loss: 0.3507 - val_binary_accuracy: 0.8333\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 214us/step - loss: 0.2315 - binary_accuracy: 0.9192 - val_loss: 0.3374 - val_binary_accuracy: 0.8389\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 187us/step - loss: 0.2186 - binary_accuracy: 0.9247 - val_loss: 0.3313 - val_binary_accuracy: 0.8611\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 178us/step - loss: 0.1946 - binary_accuracy: 0.9372 - val_loss: 0.3375 - val_binary_accuracy: 0.8722\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 178us/step - loss: 0.1875 - binary_accuracy: 0.9482 - val_loss: 0.3363 - val_binary_accuracy: 0.8556\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 178us/step - loss: 0.1783 - binary_accuracy: 0.9523 - val_loss: 0.3522 - val_binary_accuracy: 0.8667\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 171us/step - loss: 0.1671 - binary_accuracy: 0.9537 - val_loss: 0.3402 - val_binary_accuracy: 0.8778\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 202us/step - loss: 0.1636 - binary_accuracy: 0.9482 - val_loss: 0.3575 - val_binary_accuracy: 0.8667\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 179us/step - loss: 0.1447 - binary_accuracy: 0.9593 - val_loss: 0.3512 - val_binary_accuracy: 0.8889\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 0s 178us/step - loss: 0.1390 - binary_accuracy: 0.9606 - val_loss: 0.3406 - val_binary_accuracy: 0.8889\n",
      "Epoch 18/200\n",
      "1448/1448 [==============================] - 0s 176us/step - loss: 0.1278 - binary_accuracy: 0.9710 - val_loss: 0.3512 - val_binary_accuracy: 0.8944\n",
      "Epoch 19/200\n",
      "1448/1448 [==============================] - 0s 178us/step - loss: 0.1372 - binary_accuracy: 0.9620 - val_loss: 0.3729 - val_binary_accuracy: 0.8944\n",
      "Epoch 20/200\n",
      "1448/1448 [==============================] - 0s 180us/step - loss: 0.1305 - binary_accuracy: 0.9689 - val_loss: 0.3555 - val_binary_accuracy: 0.8889\n",
      "1448/1448 [==============================] - 1s 417us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "180/180 [==============================] - 0s 61us/step\n",
      "(180,)\n",
      "(180,)\n",
      "182/182 [==============================] - 0s 49us/step\n",
      "(182,)\n",
      "(182,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 41, 8)             200       \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 2632      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,977\n",
      "Trainable params: 2,961\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1448 samples, validate on 182 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 3s 2ms/step - loss: 0.6869 - binary_accuracy: 0.6181 - val_loss: 0.6509 - val_binary_accuracy: 0.6429\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 243us/step - loss: 0.5437 - binary_accuracy: 0.7417 - val_loss: 0.5668 - val_binary_accuracy: 0.7473\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 198us/step - loss: 0.4363 - binary_accuracy: 0.8135 - val_loss: 0.5184 - val_binary_accuracy: 0.7637\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 197us/step - loss: 0.3657 - binary_accuracy: 0.8660 - val_loss: 0.4820 - val_binary_accuracy: 0.8077\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 198us/step - loss: 0.3319 - binary_accuracy: 0.8785 - val_loss: 0.4499 - val_binary_accuracy: 0.8132\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 220us/step - loss: 0.2954 - binary_accuracy: 0.8999 - val_loss: 0.4289 - val_binary_accuracy: 0.8187\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 201us/step - loss: 0.2590 - binary_accuracy: 0.9137 - val_loss: 0.4206 - val_binary_accuracy: 0.8352\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 189us/step - loss: 0.2457 - binary_accuracy: 0.9233 - val_loss: 0.4373 - val_binary_accuracy: 0.8571\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 197us/step - loss: 0.2330 - binary_accuracy: 0.9275 - val_loss: 0.4284 - val_binary_accuracy: 0.8571\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 200us/step - loss: 0.2204 - binary_accuracy: 0.9254 - val_loss: 0.4387 - val_binary_accuracy: 0.8516\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 206us/step - loss: 0.2106 - binary_accuracy: 0.9427 - val_loss: 0.4383 - val_binary_accuracy: 0.8626\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 199us/step - loss: 0.1952 - binary_accuracy: 0.9358 - val_loss: 0.4331 - val_binary_accuracy: 0.8736\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 200us/step - loss: 0.1689 - binary_accuracy: 0.9544 - val_loss: 0.4802 - val_binary_accuracy: 0.8736\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 196us/step - loss: 0.1706 - binary_accuracy: 0.9586 - val_loss: 0.4592 - val_binary_accuracy: 0.8846\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 191us/step - loss: 0.1617 - binary_accuracy: 0.9558 - val_loss: 0.4542 - val_binary_accuracy: 0.8681\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 200us/step - loss: 0.1472 - binary_accuracy: 0.9634 - val_loss: 0.5141 - val_binary_accuracy: 0.8736\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 0s 223us/step - loss: 0.1446 - binary_accuracy: 0.9641 - val_loss: 0.4560 - val_binary_accuracy: 0.8736\n",
      "1448/1448 [==============================] - 1s 481us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "182/182 [==============================] - 0s 55us/step\n",
      "(182,)\n",
      "(182,)\n",
      "180/180 [==============================] - 0s 78us/step\n",
      "(180,)\n",
      "(180,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 41, 8)             200       \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8)                 2632      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,977\n",
      "Trainable params: 2,961\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1448 samples, validate on 180 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 3s 2ms/step - loss: 0.6919 - binary_accuracy: 0.5884 - val_loss: 0.6381 - val_binary_accuracy: 0.6944\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 205us/step - loss: 0.5451 - binary_accuracy: 0.7514 - val_loss: 0.5346 - val_binary_accuracy: 0.7389\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 218us/step - loss: 0.4313 - binary_accuracy: 0.8280 - val_loss: 0.4448 - val_binary_accuracy: 0.8222\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 220us/step - loss: 0.3650 - binary_accuracy: 0.8557 - val_loss: 0.4185 - val_binary_accuracy: 0.8333\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 223us/step - loss: 0.3043 - binary_accuracy: 0.8943 - val_loss: 0.4100 - val_binary_accuracy: 0.8333\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 209us/step - loss: 0.2833 - binary_accuracy: 0.9040 - val_loss: 0.3513 - val_binary_accuracy: 0.8944\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 211us/step - loss: 0.2438 - binary_accuracy: 0.9296 - val_loss: 0.3333 - val_binary_accuracy: 0.9000\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 207us/step - loss: 0.2242 - binary_accuracy: 0.9351 - val_loss: 0.3287 - val_binary_accuracy: 0.9111\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 208us/step - loss: 0.2173 - binary_accuracy: 0.9378 - val_loss: 0.3358 - val_binary_accuracy: 0.9111\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 214us/step - loss: 0.1955 - binary_accuracy: 0.9427 - val_loss: 0.3589 - val_binary_accuracy: 0.8889\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 210us/step - loss: 0.1764 - binary_accuracy: 0.9461 - val_loss: 0.3458 - val_binary_accuracy: 0.8944\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 205us/step - loss: 0.1771 - binary_accuracy: 0.9503 - val_loss: 0.4067 - val_binary_accuracy: 0.8611\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 216us/step - loss: 0.1698 - binary_accuracy: 0.9558 - val_loss: 0.3748 - val_binary_accuracy: 0.9056\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 201us/step - loss: 0.1556 - binary_accuracy: 0.9551 - val_loss: 0.3695 - val_binary_accuracy: 0.8944\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 237us/step - loss: 0.1464 - binary_accuracy: 0.9696 - val_loss: 0.4066 - val_binary_accuracy: 0.8833\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 205us/step - loss: 0.1319 - binary_accuracy: 0.9703 - val_loss: 0.4299 - val_binary_accuracy: 0.8778\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 0s 229us/step - loss: 0.1338 - binary_accuracy: 0.9648 - val_loss: 0.3800 - val_binary_accuracy: 0.8944\n",
      "Epoch 18/200\n",
      "1448/1448 [==============================] - 0s 238us/step - loss: 0.1235 - binary_accuracy: 0.9703 - val_loss: 0.4341 - val_binary_accuracy: 0.8944\n",
      "1448/1448 [==============================] - 1s 518us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "180/180 [==============================] - 0s 50us/step\n",
      "(180,)\n",
      "(180,)\n",
      "182/182 [==============================] - 0s 77us/step\n",
      "(182,)\n",
      "(182,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 41, 8)             200       \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 8)                 2632      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,977\n",
      "Trainable params: 2,961\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1448 samples, validate on 182 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 3s 2ms/step - loss: 0.6686 - binary_accuracy: 0.6215 - val_loss: 0.6246 - val_binary_accuracy: 0.6868\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 226us/step - loss: 0.5163 - binary_accuracy: 0.7686 - val_loss: 0.5492 - val_binary_accuracy: 0.7527\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 228us/step - loss: 0.4224 - binary_accuracy: 0.8481 - val_loss: 0.5116 - val_binary_accuracy: 0.7692\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 228us/step - loss: 0.3501 - binary_accuracy: 0.8764 - val_loss: 0.5050 - val_binary_accuracy: 0.7857\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 231us/step - loss: 0.3140 - binary_accuracy: 0.8847 - val_loss: 0.4878 - val_binary_accuracy: 0.7967\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 247us/step - loss: 0.2768 - binary_accuracy: 0.9075 - val_loss: 0.5214 - val_binary_accuracy: 0.8077\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 231us/step - loss: 0.2599 - binary_accuracy: 0.9171 - val_loss: 0.5041 - val_binary_accuracy: 0.8297\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 222us/step - loss: 0.2304 - binary_accuracy: 0.9365 - val_loss: 0.5479 - val_binary_accuracy: 0.8132\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 219us/step - loss: 0.2071 - binary_accuracy: 0.9399 - val_loss: 0.5167 - val_binary_accuracy: 0.8242\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 229us/step - loss: 0.1928 - binary_accuracy: 0.9448 - val_loss: 0.5001 - val_binary_accuracy: 0.8187\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 239us/step - loss: 0.1839 - binary_accuracy: 0.9586 - val_loss: 0.5290 - val_binary_accuracy: 0.8187\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 231us/step - loss: 0.1917 - binary_accuracy: 0.9454 - val_loss: 0.5188 - val_binary_accuracy: 0.8352\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 224us/step - loss: 0.1711 - binary_accuracy: 0.9620 - val_loss: 0.5134 - val_binary_accuracy: 0.8516\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 232us/step - loss: 0.1674 - binary_accuracy: 0.9599 - val_loss: 0.5186 - val_binary_accuracy: 0.8462\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 227us/step - loss: 0.1585 - binary_accuracy: 0.9641 - val_loss: 0.5523 - val_binary_accuracy: 0.8571\n",
      "1448/1448 [==============================] - 1s 571us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "182/182 [==============================] - 0s 82us/step\n",
      "(182,)\n",
      "(182,)\n",
      "180/180 [==============================] - 0s 172us/step\n",
      "(180,)\n",
      "(180,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 41, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 41, 8)             104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 41, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 41, 8)             200       \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 328)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 8)                 2632      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,977\n",
      "Trainable params: 2,961\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1448 samples, validate on 180 samples\n",
      "Epoch 1/200\n",
      "1448/1448 [==============================] - 3s 2ms/step - loss: 0.7100 - binary_accuracy: 0.5698 - val_loss: 0.5865 - val_binary_accuracy: 0.7889\n",
      "Epoch 2/200\n",
      "1448/1448 [==============================] - 0s 320us/step - loss: 0.5276 - binary_accuracy: 0.7445 - val_loss: 0.3873 - val_binary_accuracy: 0.8500\n",
      "Epoch 3/200\n",
      "1448/1448 [==============================] - 0s 238us/step - loss: 0.4002 - binary_accuracy: 0.8488 - val_loss: 0.3629 - val_binary_accuracy: 0.8833\n",
      "Epoch 4/200\n",
      "1448/1448 [==============================] - 0s 249us/step - loss: 0.3472 - binary_accuracy: 0.8660 - val_loss: 0.3345 - val_binary_accuracy: 0.8667\n",
      "Epoch 5/200\n",
      "1448/1448 [==============================] - 0s 264us/step - loss: 0.3195 - binary_accuracy: 0.8764 - val_loss: 0.2986 - val_binary_accuracy: 0.8889\n",
      "Epoch 6/200\n",
      "1448/1448 [==============================] - 0s 241us/step - loss: 0.2879 - binary_accuracy: 0.8909 - val_loss: 0.2477 - val_binary_accuracy: 0.9222\n",
      "Epoch 7/200\n",
      "1448/1448 [==============================] - 0s 238us/step - loss: 0.2610 - binary_accuracy: 0.9137 - val_loss: 0.2513 - val_binary_accuracy: 0.9000\n",
      "Epoch 8/200\n",
      "1448/1448 [==============================] - 0s 235us/step - loss: 0.2353 - binary_accuracy: 0.9254 - val_loss: 0.2424 - val_binary_accuracy: 0.9000\n",
      "Epoch 9/200\n",
      "1448/1448 [==============================] - 0s 240us/step - loss: 0.2283 - binary_accuracy: 0.9240 - val_loss: 0.2144 - val_binary_accuracy: 0.9611\n",
      "Epoch 10/200\n",
      "1448/1448 [==============================] - 0s 260us/step - loss: 0.2212 - binary_accuracy: 0.9351 - val_loss: 0.2370 - val_binary_accuracy: 0.9000\n",
      "Epoch 11/200\n",
      "1448/1448 [==============================] - 0s 242us/step - loss: 0.2041 - binary_accuracy: 0.9413 - val_loss: 0.1992 - val_binary_accuracy: 0.9222\n",
      "Epoch 12/200\n",
      "1448/1448 [==============================] - 0s 240us/step - loss: 0.1884 - binary_accuracy: 0.9496 - val_loss: 0.1770 - val_binary_accuracy: 0.9556\n",
      "Epoch 13/200\n",
      "1448/1448 [==============================] - 0s 251us/step - loss: 0.1817 - binary_accuracy: 0.9482 - val_loss: 0.2169 - val_binary_accuracy: 0.9111\n",
      "Epoch 14/200\n",
      "1448/1448 [==============================] - 0s 266us/step - loss: 0.1656 - binary_accuracy: 0.9565 - val_loss: 0.1664 - val_binary_accuracy: 0.9556\n",
      "Epoch 15/200\n",
      "1448/1448 [==============================] - 0s 245us/step - loss: 0.1691 - binary_accuracy: 0.9530 - val_loss: 0.1895 - val_binary_accuracy: 0.9333\n",
      "Epoch 16/200\n",
      "1448/1448 [==============================] - 0s 241us/step - loss: 0.1610 - binary_accuracy: 0.9586 - val_loss: 0.1457 - val_binary_accuracy: 0.9722\n",
      "Epoch 17/200\n",
      "1448/1448 [==============================] - 0s 240us/step - loss: 0.1480 - binary_accuracy: 0.9599 - val_loss: 0.1503 - val_binary_accuracy: 0.9722\n",
      "Epoch 18/200\n",
      "1448/1448 [==============================] - 0s 247us/step - loss: 0.1410 - binary_accuracy: 0.9627 - val_loss: 0.1769 - val_binary_accuracy: 0.9333\n",
      "Epoch 19/200\n",
      "1448/1448 [==============================] - 0s 241us/step - loss: 0.1439 - binary_accuracy: 0.9627 - val_loss: 0.1456 - val_binary_accuracy: 0.9778\n",
      "Epoch 20/200\n",
      "1448/1448 [==============================] - 0s 273us/step - loss: 0.1336 - binary_accuracy: 0.9682 - val_loss: 0.1629 - val_binary_accuracy: 0.9556\n",
      "Epoch 21/200\n",
      "1448/1448 [==============================] - 0s 240us/step - loss: 0.1297 - binary_accuracy: 0.9703 - val_loss: 0.1391 - val_binary_accuracy: 0.9722\n",
      "Epoch 22/200\n",
      "1448/1448 [==============================] - 0s 264us/step - loss: 0.1353 - binary_accuracy: 0.9606 - val_loss: 0.1586 - val_binary_accuracy: 0.9389\n",
      "Epoch 23/200\n",
      "1448/1448 [==============================] - 0s 237us/step - loss: 0.1368 - binary_accuracy: 0.9634 - val_loss: 0.1433 - val_binary_accuracy: 0.9778\n",
      "Epoch 24/200\n",
      "1448/1448 [==============================] - 0s 246us/step - loss: 0.1282 - binary_accuracy: 0.9655 - val_loss: 0.1484 - val_binary_accuracy: 0.9722\n",
      "Epoch 25/200\n",
      "1448/1448 [==============================] - 0s 240us/step - loss: 0.1086 - binary_accuracy: 0.9724 - val_loss: 0.1597 - val_binary_accuracy: 0.9611\n",
      "Epoch 26/200\n",
      "1448/1448 [==============================] - 0s 244us/step - loss: 0.1122 - binary_accuracy: 0.9682 - val_loss: 0.1644 - val_binary_accuracy: 0.9389\n",
      "Epoch 27/200\n",
      "1448/1448 [==============================] - 0s 254us/step - loss: 0.1161 - binary_accuracy: 0.9682 - val_loss: 0.1484 - val_binary_accuracy: 0.9556\n",
      "Epoch 28/200\n",
      "1448/1448 [==============================] - 0s 245us/step - loss: 0.1064 - binary_accuracy: 0.9738 - val_loss: 0.1624 - val_binary_accuracy: 0.9556\n",
      "Epoch 29/200\n",
      "1448/1448 [==============================] - 0s 239us/step - loss: 0.1029 - binary_accuracy: 0.9765 - val_loss: 0.1582 - val_binary_accuracy: 0.9556\n",
      "Epoch 30/200\n",
      "1448/1448 [==============================] - 0s 240us/step - loss: 0.1108 - binary_accuracy: 0.9738 - val_loss: 0.1682 - val_binary_accuracy: 0.9722\n",
      "Epoch 31/200\n",
      "1448/1448 [==============================] - 0s 275us/step - loss: 0.1021 - binary_accuracy: 0.9765 - val_loss: 0.1774 - val_binary_accuracy: 0.9333\n",
      "1448/1448 [==============================] - 1s 630us/step\n",
      "(1448,)\n",
      "(1448,)\n",
      "180/180 [==============================] - 0s 144us/step\n",
      "(180,)\n",
      "(180,)\n",
      "182/182 [==============================] - 0s 88us/step\n",
      "(182,)\n",
      "(182,)\n"
     ]
    }
   ],
   "source": [
    "All_data = 'G.subterraneus.fasta.txt' \n",
    "OutputDir = 'C:/Users/NSCL/N4-methylctosine//outputs/'\n",
    "funciton(All_data, OutputDir, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
